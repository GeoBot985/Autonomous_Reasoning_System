import logging
import requests
import json
import re
import datetime
import time
from typing import Optional, List

from .memory import get_memory_system
from .retrieval import RetrievalSystem

logger = logging.getLogger("ARS_Brain")

# Config defaults
OLLAMA_BASE = "http://localhost:11434/api"
DEFAULT_MODEL = "gemma3:1b" 

class LLMEngine:
    def __init__(self, model=DEFAULT_MODEL, base_url=OLLAMA_BASE):
        self.model = model
        self.generate_url = f"{base_url}/generate"
        self.tags_url = f"{base_url}/tags"
        print(f"[Brain]  ðŸ§  LLM Engine configured: {model}")
        self._check_model_exists()
        self._warmup()

    def _check_model_exists(self):
        print(f"[Brain]  ðŸ” Checking if model '{self.model}' exists locally...")
        try:
            resp = requests.get(self.tags_url, timeout=5)
            if resp.status_code == 200:
                models = [m['name'] for m in resp.json().get('models', [])]
                if any(self.model in m for m in models):
                    print(f"[Brain]  âœ… Model '{self.model}' found.")
                else:
                    print(f"[Brain]  âš ï¸ WARNING: Model '{self.model}' not found in Ollama!")
        except Exception as e:
            print(f"[Brain]  âš ï¸ Could not list models: {e}")

    def _warmup(self):
        print(f"[Brain]  ðŸ”¥ Warming up LLM...")
        try:
            requests.post(self.generate_url, json={"model": self.model, "prompt": "hi", "stream": False, "keep_alive": "5m"}, timeout=10)
            print(f"[Brain]  âœ… LLM Warmed up.")
        except Exception as e:
            print(f"[Brain]  âš ï¸ Warmup failed: {e}")

    def generate(self, prompt: str, system: str = None, temperature: float = 0.7, format: str = None) -> str:
        full_prompt = prompt
        if system:
            full_prompt = f"SYSTEM: {system}\n\nUSER: {prompt}"
        
        payload = { 
            "model": self.model, 
            "prompt": full_prompt, 
            "stream": False, 
            "temperature": temperature 
        }
        if format:
            payload["format"] = format

        print(f"[LLM] ðŸ“¤ Sending request to Ollama ({len(full_prompt)} chars)...")
        start_t = time.time()
        try:
            resp = requests.post(self.generate_url, json=payload, timeout=120)
            resp.raise_for_status()
            print(f"[LLM] ðŸ“¥ Response received ({time.time() - start_t:.2f}s)")
            return resp.json().get("response", "").strip()
        except Exception as e:
            logger.error(f"LLM Generation failed: {e}")
            return f"[Error: LLM unavailable - {e}]"

class Brain:
    def __init__(self):
        print("\n[Brain] ðŸŸ¢ Initializing Brain...")
        start_t = time.time()
        self.memory = get_memory_system(db_path="data/memory.duckdb")
        self.retrieval = RetrievalSystem(self.memory)
        self.llm = LLMEngine()
        self.plugins = {}
        self._register_basic_tools()
        print(f"[Brain] âœ… Brain Ready (Total startup: {time.time() - start_t:.2f}s)\n")

    def think(self, user_input: str) -> str:
        if not user_input or not user_input.strip(): return ""
        text = user_input.strip()
        
        plugin_response = self._check_plugins(text)
        if plugin_response: return plugin_response

        intent, metadata = self._classify_intent(text)
        print(f"[Brain] ðŸ§­ Intent: {intent}") 
        
        if intent == "store":
            return self._handle_storage(text, metadata)
        elif intent == "plan":
            return self._handle_planning(text)
        else:
            return self._handle_chat(text)

    def _classify_intent(self, text: str):
        lower = text.lower()
        
        # 1. Explicit Storage Commands
        if any(x in lower for x in ["remember that", "don't forget", "remind me", "save this", "note that"]):
            return "store", {"source": "direct_command"}
        
        # 2. Planning Keywords
        if any(x in lower for x in ["plan a", "create a goal", "how do i", "research"]):
            return "plan", {}

        # 3. RAG / Action Commands (The Fix!)
        # If it starts with an imperative verb, it is a request for output (Chat), not input (Store).
        rag_verbs = ["summarize", "explain", "describe", "list", "show", "find", "search", "define", "tell"]
        clean_start = lower.replace("please ", "").strip()
        if any(clean_start.startswith(v) for v in rag_verbs):
            return "chat", {}

        # 4. Question Check
        # Expanded to catch "Could you..." or "Would you..."
        is_question = lower.startswith(("when", "what", "who", "where", "how", "is ", "does", "can", "could", "would")) or "?" in lower
        
        # 5. Implicit Assertion (Store)
        # Only store if it's NOT a question AND NOT a RAG command
        if not is_question and 3 < len(lower.split()) < 20:
             return "store", {"source": "implicit_assertion"}

        return "chat", {}

    def _handle_storage(self, text: str, meta: dict) -> str:
        clean_text = re.sub(r"^(remember that|don't forget|remind me|save this|note that)\s*", "", text, flags=re.IGNORECASE)
        kg_triples = self._extract_triples_via_llm(clean_text)
        
        self.memory.remember(
            clean_text, 
            memory_type="fact", 
            importance=1.0, 
            metadata={"kg_triples": kg_triples}
        )
        
        if kg_triples:
            return f"âœ… Saved fact and extracted knowledge: {kg_triples}"
        else:
            return f"âœ… Saved: '{clean_text}'"

    def _extract_triples_via_llm(self, text: str) -> List[tuple]:
        print(f"[Brain] ðŸ•¸ï¸ Extracting KG Triples for: '{text}'")
        system = (
            "You are a Knowledge Graph extractor. Convert the user's text into a JSON list of triples. "
            "Format: [[\"subject\", \"relation\", \"object\"]].\n"
            "Rules:\n"
            "1. Use lower case.\n"
            "2. Convert possessives: \"Cornelia's birthday\" -> [\"cornelia\", \"has_birthday\", ...]\n"
            "3. Capture definitions: \"Password is X\" -> [\"password\", \"is\", \"x\"]\n"
            "4. Return ONLY the JSON list."
        )
        try:
            response = self.llm.generate(text, system=system, temperature=0.1)
            response = response.replace("```json", "").replace("```", "").strip()
            triples = json.loads(response)
            valid_triples = []
            if isinstance(triples, list):
                for t in triples:
                    if isinstance(t, list) and len(t) == 3:
                        valid_triples.append((t[0], t[1], t[2]))
            print(f"[Brain]    ðŸ•¸ï¸ Found: {valid_triples}")
            return valid_triples
        except Exception as e:
            print(f"[Brain]    âš ï¸ KG Extraction failed: {e}")
            return []

    def _handle_chat(self, text: str) -> str:
        context_str = self.retrieval.get_context_string(text)
        
        # Detect "Soft" Intent (Summarization/Explanation)
        is_summary = any(w in text.lower() for w in ["summarize", "list", "explain", "describe", "what is", "show"])
        
        if is_summary:
            # PERMISSIVE PROMPT: Encourages synthesis
            system_prompt = (
                "You are Tyrone. Use the provided CONTEXT to answer the user.\n"
                "Rules:\n"
                "1. Synthesize the information found in the CONTEXT facts.\n"
                "2. If the text is cut off or partial, summarize what is visible.\n"
                "3. Ignore facts that look like previous user commands (e.g. 'Please summarize...')."
            )
        else:
            # STRICT PROMPT: For specific fact retrieval
            system_prompt = (
                "You are Tyrone. Use the provided CONTEXT to answer the user.\n"
                "Rules:\n"
                "1. FACTS in the context are absolute truth.\n"
                "2. Do not guess. If the specific answer is missing, say you don't know."
            )
            
        return self.llm.generate(text, system=f"{system_prompt}\n\n{context_str}")

    def _handle_planning(self, text: str) -> str:
        try:
            from .plan_builder import get_planner
            return get_planner(self.memory, self.llm).process_request(text)
        except Exception as e:
            return f"âš ï¸ Planning error: {e}"

    def _register_basic_tools(self):
        self.plugins["time"] = lambda x: f"The current time is {datetime.datetime.now().strftime('%H:%M')}."
        self.plugins["date"] = lambda x: f"Today is {datetime.datetime.now().strftime('%A, %d %B %Y')}."
        
    def _check_plugins(self, text: str) -> Optional[str]:
        return self.plugins.get(text.lower().strip())

_brain_instance = None
def get_brain():
    global _brain_instance
    if _brain_instance is None: _brain_instance = Brain()
    return _brain_instance

if __name__ == "__main__":
    b = get_brain()
    print("ðŸ§  Tyrone Refactored (CLI Mode)")
    while True:
        try:
            q = input("You> ")
            if q.lower() in ["exit", "quit"]: break
            print(f"Tyrone> {b.think(q)}")
        except KeyboardInterrupt:
            break
