
# ===========================================================================
# FILE START: migrate_to_production.py
# ===========================================================================

import os
import shutil
import sys
from pathlib import Path
from datetime import datetime

# --- CONFIGURATION ---
# Assuming script is run from 'src/'
BASE_DIR = Path(__file__).parent
PACKAGE_DIR = BASE_DIR / "Autonomous_Reasoning_System"
REFACTOR_DIR = PACKAGE_DIR / "refactor"
DATA_DIR = PACKAGE_DIR / "data"

# Folders to DELETE (Legacy)
LEGACY_DIRS = [
    "control", "cognition", "planning", "llm", 
    "memory", "rag", "io", "tools", "infrastructure", 
    "tests", "__pycache__"
]

# Files to DELETE (Legacy)
LEGACY_FILES = [
    "consolidated_app.py", "main.py", "init_runtime.py", 
    "interface.py" # Old interface if present in root
]

def create_backup():
    """Zips the current package before we destroy it."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"backup_legacy_{timestamp}"
    print(f"üì¶ Creating backup: {backup_name}.zip ...")
    shutil.make_archive(backup_name, 'zip', PACKAGE_DIR)
    print("‚úÖ Backup complete.")

def purge_legacy():
    """Deletes old folders and files."""
    print("üî• Purging legacy code...")
    
    # Delete Directories
    for folder in LEGACY_DIRS:
        target = PACKAGE_DIR / folder
        if target.exists() and target.is_dir():
            try:
                shutil.rmtree(target)
                print(f"   Deleted folder: {folder}/")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Failed to delete {folder}: {e}")

    # Delete Files
    for file in LEGACY_FILES:
        target = PACKAGE_DIR / file
        if target.exists() and target.is_file():
            try:
                target.unlink()
                print(f"   Deleted file: {file}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Failed to delete {file}: {e}")

def promote_refactor():
    """Moves files from refactor/ to package root."""
    print("üöÄ Promoting refactored code...")
    
    if not REFACTOR_DIR.exists():
        print("‚ùå Error: 'refactor' directory not found!")
        sys.exit(1)

    # Move everything from refactor/ to Autonomous_Reasoning_System/
    for item in REFACTOR_DIR.iterdir():
        if item.name == "__pycache__":
            continue
            
        target = PACKAGE_DIR / item.name
        
        if target.exists():
            print(f"   ‚ö†Ô∏è Overwriting existing: {item.name}")
            if target.is_dir():
                shutil.rmtree(target)
            else:
                target.unlink()
        
        shutil.move(str(item), str(target))
        print(f"   Moved: {item.name}")

    # Remove empty refactor dir
    try:
        REFACTOR_DIR.rmdir()
        print("   Cleaned up refactor/ directory.")
    except Exception:
        print("   (Note: refactor/ dir not empty, kept it just in case)")

def main():
    print(f"--- TYRONE MIGRATION TOOL ---")
    print(f"Target Package: {PACKAGE_DIR}")
    
    if input("Are you sure you want to replace the codebase? (y/n): ").lower() != 'y':
        print("Aborted.")
        return

    create_backup()
    purge_legacy()
    promote_refactor()
    
    print("\n‚ú® Migration Successful!")
    print("You can now run the app using:")
    print("python -m Autonomous_Reasoning_System.interface")

if __name__ == "__main__":
    main()


# ===========================================================================
# FILE START: Autonomous_Reasoning_System\brain.py
# ===========================================================================

import logging
import json
import re
import datetime
import time
import threading
from typing import Optional, List, Tuple, Dict, Any

import requests

from . import config
from .memory import get_memory_system
from .tools.web_search import perform_google_search
from .retrieval import RetrievalSystem
from .reflection import get_reflector

logger = logging.getLogger("ARS_Brain")

# Config defaults (fall back to config module for host/model)
OLLAMA_BASE = getattr(config, "OLLAMA_API_BASE", "http://localhost:11434/api")
DEFAULT_MODEL = getattr(config, "LLM_MODEL", "granite4:1b")
TAGS_TIMEOUT = 5
WARMUP_TIMEOUT = 15
REQUEST_TIMEOUT = 90

class LLMEngine:
    def __init__(self, model: str = DEFAULT_MODEL, api_base: str = OLLAMA_BASE):
        self.model = model
        self.api_base = api_base.rstrip('/')
        self.tags_url = f"{self.api_base}/tags"
        self.generate_url = f"{self.api_base}/generate"
        
        # Use a persistent session for better connection handling
        self.session = requests.Session()
        self.session.trust_env = False  # avoid proxy/env interference for local Ollama
        
        self._check_model_exists()
        self._warmup()

    def ping(self, timeout: int = 2) -> bool:
        """Quick health check against the tags endpoint."""
        try:
            self.session.get(self.tags_url, timeout=timeout).raise_for_status()
            return True
        except Exception:
            return False

    def _check_model_exists(self) -> None:
        logger.info(f"Checking if model '{self.model}' exists locally...")
        try:
            resp = self.session.get(self.tags_url, timeout=TAGS_TIMEOUT)
            if resp.status_code == 200:
                models = [m['name'] for m in resp.json().get('models', [])]
                if any(self.model in m for m in models):
                    logger.info(f"Model '{self.model}' found.")
                else:
                    logger.warning(f"Model '{self.model}' not found in Ollama!")
        except Exception as e:
            logger.warning(f"Could not list models: {e}")

    def _warmup(self) -> None:
        logger.info("Warming up LLM...")
        try:
            self.session.post(
                self.generate_url,
                json={"model": self.model, "prompt": "hi", "stream": False, "keep_alive": "5m"},
                timeout=WARMUP_TIMEOUT
            )
            logger.info("LLM Warmed up.")
        except Exception as e:
            logger.warning(f"Warmup failed: {e}")

    def generate(self, prompt: str, system: str = None, temperature: float = 0.7, format: str = None) -> str:
        full_prompt = prompt
        if system:
            full_prompt = f"SYSTEM: {system}\n\nUSER: {prompt}"

        payload = {
            "model": self.model,
            "prompt": full_prompt,
            "stream": False,
            "temperature": temperature
        }
        if format:
            payload["format"] = format

        logger.debug(f"Sending request to Ollama ({len(full_prompt)} chars)...")
        start_t = time.time()
        try:
            resp = self.session.post(self.generate_url, json=payload, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            logger.debug(f"Response received ({time.time() - start_t:.2f}s)")
            return resp.json().get("response", "").strip()
        except Exception as e:
            logger.error(f"LLM Generation failed: {e}")
            try:
                # Attempt to log detailed error info if response exists
                if 'resp' in locals() and hasattr(resp, 'status_code'):
                    logger.error(f"Ollama status={resp.status_code}, body={resp.text[:500]}")
            except UnboundLocalError:
                pass
            return f"[Error: LLM unavailable - {e}]"

class Brain:
    def __init__(self):
        logger.info("Initializing Brain...")
        start_t = time.time()

        self.memory = get_memory_system(db_path="data/memory.duckdb")
        self.retrieval = RetrievalSystem(self.memory)
        self.llm = LLMEngine()
        self.plugins: Dict[str, Any] = {}
        self.reflector = get_reflector(self.memory, self.llm)

        self._warmup_memory()
        self._start_maintenance_loop()
        self._register_basic_tools()

        logger.info(f"Brain Ready (Total startup: {time.time() - start_t:.2f}s)")

    def _warmup_memory(self) -> None:
        """Forces the RAG system to run a trivial vector search to load VSS indices and Embedder resources."""
        logger.info("Warming up RAG/Memory System...")
        start_t = time.time()
        try:
            self.retrieval.get_context_string("quick test query for memory warmup", include_history=None)
            logger.info(f"RAG/Memory Warmed up ({time.time() - start_t:.2f}s).")
        except Exception as e:
            logger.warning(f"RAG Warmup failed: {e}")

    def _run_maintenance(self) -> None:
        """Runs periodic memory decay and session consolidation."""
        while True:
            time.sleep(1800)
            logger.info("Running scheduled maintenance...")
            try:
                self.reflector.decay_importance()
                self.reflector.consolidate_sessions()
            except Exception as e:
                logger.error(f"Maintenance task failed: {e}")

    def _start_maintenance_loop(self) -> None:
        """Starts the maintenance thread in the background."""
        t = threading.Thread(target=self._run_maintenance, daemon=True)
        t.start()

    def think(self, user_input: str, history: List[dict] = None) -> str:
        if not user_input or not user_input.strip():
            return ""

        text = user_input.strip()

        if self._is_web_search_query(text):
            return self._handle_web_search(text)
        
        plugin_response = self._check_plugins(text)
        if plugin_response:
            return plugin_response

        intent, metadata = self._classify_intent(text)
        logger.info(f"Intent: {intent}")
        
        if intent == "store":
            return self._handle_storage(text, metadata)
        elif intent == "plan":
            return self._handle_planning(text)
        else:
            return self._handle_chat(text, history)
        
    def _format_history(self, history: List[dict]) -> List[str]:
        """Converts the list of dicts history into a list of strings for context."""
        formatted = []
        if not history:
            return []
        
        # Exclude the very last entry, as that is the user's current message
        previous_turns = history[:-1] 

        for turn in previous_turns:
            role = turn.get('role', '').capitalize()
            content = turn.get('content', '')
            formatted.append(f"{role}: {content}")
        return formatted

    def _classify_intent(self, text: str) -> Tuple[str, dict]:
        lower = text.lower()
        
        # 1. Explicit Storage Commands
        storage_keywords = ["remember that", "don't forget", "remind me", "save this", "note that"]
        if any(x in lower for x in storage_keywords):
            return "store", {"source": "direct_command"}
        
        # 2. Planning Keywords
        planning_keywords = ["plan a", "create a goal", "how do i", "research"]
        if any(x in lower for x in planning_keywords):
            return "plan", {}

        # 3. RAG / Action Commands
        if lower.startswith("web search"):
            return "chat", {}

        rag_verbs = ["summarize", "explain", "describe", "list", "show", "find", "search", "define", "tell"]
        clean_start = lower.replace("please ", "").strip()
        if any(clean_start.startswith(v) for v in rag_verbs):
            return "chat", {}

        # 4. Question Check
        question_starters = ("when", "what", "who", "where", "how", "is ", "does", "can", "could", "would")
        is_question = lower.startswith(question_starters) or "?" in lower
        
        # 5. Implicit Assertion (Store)
        if not is_question and 3 < len(lower.split()) < 20:
             return "store", {"source": "implicit_assertion"}

        return "chat", {}

    def _handle_storage(self, text: str, meta: dict) -> str:
        clean_text = re.sub(
            r"^(remember that|don't forget|remind me|save this|note that)\s*",
            "",
            text,
            flags=re.IGNORECASE
        )
        kg_triples = self._extract_triples_via_llm(clean_text)
        
        self.memory.remember(
            clean_text, 
            memory_type="fact", 
            importance=1.0, 
            metadata={"kg_triples": kg_triples}
        )
        
        if kg_triples:
            return f"‚úÖ Saved fact and extracted knowledge: {kg_triples}"
        else:
            return f"‚úÖ Saved: '{clean_text}'"

    def _is_web_search_query(self, text: str) -> bool:
        lower = text.lower().strip()
        return lower.startswith("web search") or lower.startswith("search web")

    def _handle_web_search(self, text: str) -> str:
        query = text
        lower = text.lower()

        if ":" in text:
            prefix, remainder = text.split(":", 1)
            if prefix.lower().strip() in {"web search", "search web"}:
                query = remainder.strip()
        elif lower.startswith("web search"):
            query = text[len("web search"):].strip()
        elif lower.startswith("search web"):
            query = text[len("search web"):].strip()

        query = query.strip()
        if not query:
            return "Please tell me what you'd like me to search for."

        try:
            result = perform_google_search(query)
            return f"Web search result for '{query}':\n{result}"
        except Exception as exc:
            logger.error(f"Web search failed: {exc}", exc_info=True)
            return "I tried to perform the web search but something went wrong."

    def _extract_triples_via_llm(self, text: str) -> List[tuple]:
        logger.debug(f"Extracting KG Triples for: '{text}'")
        system = (
            "You are a Knowledge Graph extractor. Convert the user's text into a JSON list of triples. "
            "Format: [[\"subject\", \"relation\", \"object\"]].\n"
            "Rules:\n"
            "1. Use lower case.\n"
            "2. Convert possessives: \"Cornelia's birthday\" -> [\"cornelia\", \"has_birthday\", ...]\n"
            "3. Capture definitions: \"Password is X\" -> [\"password\", \"is\", \"x\"]\n"
            "4. Return ONLY the JSON list."
        )
        try:
            response = self.llm.generate(text, system=system, temperature=0.1)
            response = response.replace("```json", "").replace("```", "").strip()
            triples = json.loads(response)
            valid_triples = []
            if isinstance(triples, list):
                for t in triples:
                    if isinstance(t, list) and len(t) == 3:
                        valid_triples.append((t[0], t[1], t[2]))
            logger.debug(f"Found: {valid_triples}")
            return valid_triples
        except Exception as e:
            logger.warning(f"KG Extraction failed: {e}")
            return []

    def _handle_chat(self, text: str, history: List[dict] = None) -> str:
        # 1. Check for specific document request
        doc_match = re.search(r"(summarize|explain|show me).*?(\w+\.\w+)", text, re.IGNORECASE)
        
        if doc_match:
            action = doc_match.group(1).lower()
            filename = doc_match.group(2)
            
            full_doc = self.memory.get_whole_document(filename)
            
            # Context window safeguard (15,000 chars is conservative for a 3B model)
            if full_doc and len(full_doc) < 15000: 
                context_str = f"### FULL DOCUMENT SOURCE: {filename} ###\n{full_doc}"
                # Rephrase the user query to tell the LLM what to do with the *provided content*
                text = f"{action} the provided document content." 
                logger.info(f"Using full document '{filename}' as context (Size: {len(full_doc)} chars).")
            else:
                formatted_history = self._format_history(history)
                context_str = self.retrieval.get_context_string(text, include_history=formatted_history)
        
        else:
            formatted_history = self._format_history(history)
            context_str = self.retrieval.get_context_string(text, include_history=formatted_history)
        
        is_summary = any(w in text.lower() for w in ["summarize", "list", "explain", "describe", "what is", "show"])
        
        if is_summary:
            system_prompt = (
                "You are Tyrone. Use the provided CONTEXT to answer the user.\n"
                "Rules:\n"
                "1. Synthesize the information found in the CONTEXT facts.\n"
                "2. If the text is cut off or partial, summarize what is visible.\n"
                "3. Ignore facts that look like previous user commands (e.g. 'Please summarize...')."
            )
        else:
            system_prompt = (
                "You are Tyrone. Use the provided CONTEXT to answer the user.\n"
                "Rules:\n"
                "1. FACTS in the context are absolute truth.\n"
                "2. Do not guess. If the specific answer is missing, say you don't know."
            )
            
        return self.llm.generate(text, system=f"{system_prompt}\n\n{context_str}")

    def _handle_planning(self, text: str) -> str:
        try:
            logger.info("Loading Planner...")
            # Late import to avoid circular dependency
            from .plan_builder import get_planner
            logger.info(f"Delegating to Planner: '{text}'")
            return get_planner(self.memory, self.llm, self.retrieval).process_request(text)
        except Exception as e:
            return f"‚ö†Ô∏è Planning error: {e}"

    def _register_basic_tools(self) -> None:
        self.plugins["time"] = lambda x: f"The current time is {datetime.datetime.now().strftime('%H:%M')}."
        self.plugins["date"] = lambda x: f"Today is {datetime.datetime.now().strftime('%A, %d %B %Y')}."
        
    def _check_plugins(self, text: str) -> Optional[str]:
        return self.plugins.get(text.lower().strip())

_brain_instance = None

def get_brain() -> Brain:
    global _brain_instance
    if _brain_instance is None:
        _brain_instance = Brain()
    return _brain_instance

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    b = get_brain()
    print("üß† Tyrone Refactored (CLI Mode)")
    while True:
        try:
            q = input("You> ")
            if q.lower() in ["exit", "quit"]:
                break
            print(f"Tyrone> {b.think(q)}")
        except KeyboardInterrupt:
            break



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\clean_poison.py
# ===========================================================================

import duckdb
import os

db_path = "data/memory.duckdb"

print(f"üßπ Checking database at {db_path}...")

try:
    con = duckdb.connect(db_path)
    
    # Check if table exists
    table_exists = con.execute(
        "SELECT count(*) FROM information_schema.tables WHERE table_name = 'memory'"
    ).fetchone()[0] > 0

    if not table_exists:
        print("‚úÖ Table 'memory' does not exist yet. Database is fresh/empty.")
        print("   (You can skip this step and run the interface directly.)")
    else:
        print("üîç Scanning for poisoned rows...")
        poison_patterns = [
            "Please summarize%",
            "Summarize my%",
            "Can you summarize%",
            "What can you tell%",
            "Describe%"
        ]

        count = 0
        for p in poison_patterns:
            n = con.execute("SELECT count(*) FROM memory WHERE text ILIKE ?", (p,)).fetchone()[0]
            if n > 0:
                print(f"   Found {n} bad rows matching '{p}'")
                con.execute("DELETE FROM memory WHERE text ILIKE ?", (p,))
                count += n

        # Clean orphaned vectors
        if count > 0:
            con.execute("DELETE FROM vectors WHERE id NOT IN (SELECT id FROM memory)")
            print(f"‚úÖ Removed {count} bad memories and cleaned vectors.")
        else:
            print("‚úÖ Database is clean. No poisoned commands found.")

except Exception as e:
    print(f"‚ö†Ô∏è Error accessing DB: {e}")


# ===========================================================================
# FILE START: Autonomous_Reasoning_System\config.py
# ===========================================================================

import os
from pathlib import Path

# --- Project Paths ---
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"

# Ensure data directory exists
DATA_DIR.mkdir(parents=True, exist_ok=True)

# --- Database ---
MEMORY_DB_PATH = os.getenv("ARS_MEMORY_DB_PATH", str(DATA_DIR / "memory.duckdb"))

# --- Ollama (LLM) ---
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_API_BASE = f"{OLLAMA_HOST}/api"
#LLM_MODEL = os.getenv("ARS_LLM_MODEL", "gemma3:1b")
LLM_MODEL = os.getenv("ARS_LLM_MODEL", "granite4:1b")


# --- Embeddings (FastEmbed) ---
EMBEDDING_MODEL_NAME = "BAAI/bge-small-en-v1.5"  # Official, 384-dim, ONNX auto-download, ~500 MB but fast on CPU
VECTOR_DIMENSION = 384



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\debug_vectors.py
# ===========================================================================

import duckdb
from fastembed import TextEmbedding

# Initialize
print("Loading model...")
embedder = TextEmbedding(model_name="BAAI/bge-small-en-v1.5")
con = duckdb.connect("data/memory.duckdb")

# 1. Check Content
print("\n--- 1. Database Content ---")
memories = con.execute("SELECT id, text FROM memory").fetchall()
for m in memories:
    print(f"[{m[0][:4]}] {m[1]}")

# 2. Check Vector Search Scores
query = "When is Cornelia's birthday?"
print(f"\n--- 2. Vector Scores for: '{query}' ---")
query_vec = list(embedder.embed([query]))[0].tolist()

# We run the query with threshold 0.0 to see EVERYTHING
try:
    # Load VSS
    con.execute("INSTALL vss; LOAD vss;")
    results = con.execute(f"""
        SELECT m.text, (1 - list_cosine_similarity(v.embedding, ?::FLOAT[384])) as score
        FROM vectors v
        JOIN memory m ON v.id = m.id
        ORDER BY score DESC
    """, (query_vec,)).fetchall()

    for r in results:
        print(f"Score: {r[1]:.4f} | Text: {r[0]}")
except Exception as e:
    print(f"Vector search failed: {e}")


# ===========================================================================
# FILE START: Autonomous_Reasoning_System\interface.py
# ===========================================================================

import gradio as gr
import logging
import sys
import threading
import time
import textwrap
import signal
import os
import re
from pathlib import Path

_processing_files = set()   # ‚Üê Global deduplication lock

# Try importing pypdf
try:
    from pypdf import PdfReader
    HAS_PDF = True
except ImportError:
    HAS_PDF = False

from .brain import get_brain

# --- CTRL+C HANDLER ---
def force_exit(signum, frame):
    os._exit(0)
signal.signal(signal.SIGINT, force_exit)
signal.signal(signal.SIGTERM, force_exit)

# --- Logs ---
class ListHandler(logging.Handler):
    def __init__(self):
        super().__init__()
        self.log_buffer = []
        self.lock = threading.Lock()
    def emit(self, record):
        try:
            msg = self.format(record)
            with self.lock:
                self.log_buffer.append(msg)
                if len(self.log_buffer) > 200: self.log_buffer.pop(0)
        except: pass
    def get_logs_as_str(self):
        with self.lock: return "\n".join(reversed(self.log_buffer))

root = logging.getLogger()
root.setLevel(logging.INFO)
if root.handlers:
    for h in root.handlers: root.removeHandler(h)
log_capture = ListHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
log_capture.setFormatter(formatter)
root.addHandler(log_capture)
console = logging.StreamHandler(sys.stdout)
console.setFormatter(formatter)
root.addHandler(console)
logger = logging.getLogger("Interface")

print("üß† Initializing Refactored Brain...")
brain = get_brain()

def chat_interaction(user_message, history):
    # Retrieve the brain instance (initializes only once per process)
    local_brain = get_brain()
    
    print(f"\n[UI] üì® Received: '{user_message}'")
    if not user_message: return "", history
    if history is None: history = []
    history.append({"role": "user", "content": user_message})
    
    print(f"[UI] ‚è≥ Sending to Brain...")
    start_t = time.time()
    try:
        response_text = local_brain.think(user_message, history)
        print(f"[UI] ‚úÖ Brain responded in {time.time() - start_t:.2f}s")
    except Exception as e:
        logger.error(f"Brain Error: {e}", exc_info=True)
        response_text = f"‚ö†Ô∏è Error: {e}"
    
    history.append({"role": "assistant", "content": response_text})
    return "", history

# Global headers regex for CV-style documents
# We look for common headers like EXPERIENCE, SKILLS, FMI, etc.
HEADER_REGEX = re.compile(
    r'^\s*(EXPERIENCE|SKILLS|EDUCATION|SUMMARY|PROFILE|FMI|PROJECTS|AWARDS|CONTACTS|HISTORY|GOALS|GOAL)\s*$', 
    re.IGNORECASE
)

def ingest_files(file_objs):
    # Retrieve the brain instance (initializes only once per process)
    local_brain = get_brain()

    global _processing_files
    
    if not file_objs:
        return "No files uploaded."

    results = []

    for file_obj in file_objs:
        path = Path(file_obj.name)

        if path in _processing_files:
            results.append(f"Already processing ‚Üí skipped: {path.name}")
            continue

        _processing_files.add(path)
        full_text = ""
        
        try:
            print(f"[UI] Ingesting {path.name}...")

            # ‚îÄ‚îÄ 1. Extract text ‚îÄ‚îÄ
            if path.suffix.lower() == ".pdf" and HAS_PDF:
                reader = PdfReader(file_obj.name)
                full_text = "".join(page.extract_text() for page in reader.pages)
            else:
                with open(file_obj.name, 'r', encoding='utf-8', errors='ignore') as f:
                    full_text = f.read()
            
            if not full_text:
                results.append(f"No usable text found in: {path.name}")
                continue

            # ‚îÄ‚îÄ 2. Segment by Header (NEW LOGIC) ‚îÄ‚îÄ
            section_segments = [] # Stores: [{'text': '...', 'section': '...'}, ...]
            current_section = "DOCUMENT HEADER"
            current_text_block = ""
            
            lines = full_text.split('\n')
            
            for line in lines:
                header_match = HEADER_REGEX.match(line)
                
                # Heuristic: Check if line matches a header pattern and is short
                if header_match and 5 < len(line.strip()) < 30: 
                    # End of previous section
                    if current_text_block.strip():
                        section_segments.append({'text': current_text_block.strip(), 'section': current_section})
                    
                    # Start of new section
                    current_section = header_match.group(1).upper()
                    current_text_block = line + "\n"
                else:
                    current_text_block += line + "\n"
            
            # Add the final block
            if current_text_block.strip():
                section_segments.append({'text': current_text_block.strip(), 'section': current_section})
            
            # ‚îÄ‚îÄ 3. Chunk Segments and Prepare Metadata ‚îÄ‚îÄ
            batch_texts = []
            metadata_list = []
            
            for segment in section_segments:
                # Chunk the section block (using the standard 500 width)
                # Setting replace_whitespace=False to prevent paragraphs merging poorly
                chunks = textwrap.wrap(segment['text'], width=500, replace_whitespace=False)
                
                for chunk in chunks:
                    batch_texts.append(chunk)
                    # Metadata now includes the section header
                    metadata_list.append({'section': segment['section']}) 

            # ‚îÄ‚îÄ 4. Save ‚îÄ‚îÄ
            if batch_texts:
                print(f"[Memory] Batch processing {len(batch_texts)} chunks...")
                local_brain.memory.remember_batch(
                    batch_texts,
                    memory_type="document_chunk",
                    importance=0.5,
                    source=path.name,
                    metadata_list=metadata_list # <-- Pass the metadata list
                )
                results.append(f"Completed: {path.name} ({len(batch_texts)} chunks)")
            else:
                results.append(f"No usable text found in: {path.name}")

        except Exception as e:
            logger.error(f"Ingest failed for {path.name}: {e}", exc_info=True)
            results.append(f"Failed: {path.name} ({str(e)})")
        finally:
            _processing_files.discard(path)

    return "\n".join(results)

def refresh_logs(): return log_capture.get_logs_as_str()

with gr.Blocks(title="Tyrone ARS") as demo:
    gr.Markdown("# üß† Tyrone ARS")
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=600, label="Interaction")
            msg = gr.Textbox(label="Command", placeholder="Type here...")
            with gr.Row():
                send = gr.Button("Send", variant="primary")
                clear = gr.Button("Clear")
        with gr.Column(scale=1):
            gr.Markdown("### üìÇ Quick Memory Ingest")
            files = gr.File(file_count="multiple", label="Upload Documents")
            upload_status = gr.Textbox(label="Status", interactive=False)
            gr.Markdown("### üñ•Ô∏è Live Logs")
            logs = gr.Code(language="shell", interactive=False, lines=20, label="System Activity")
            timer = gr.Timer(1)

    msg.submit(chat_interaction, [msg, chatbot], [msg, chatbot])
    send.click(chat_interaction, [msg, chatbot], [msg, chatbot])
    files.upload(ingest_files, files, upload_status)
    timer.tick(refresh_logs, outputs=logs)
    clear.click(lambda: [], None, chatbot, queue=False)

if __name__ == "__main__":
    # ... (all gr.Blocks definition and component bindings remain the same) ...

    # The final launch command
    print("\n[UI] üöÄ Launching Gradio. Press Ctrl+C to stop.")
    
    # --- CHANGE THIS LINE ---
    # ADDED 'show_api=False' and 'inbrowser=False' for cleaner startup, but most importantly:
    # ADDED 'prevent_thread_lock=True' and removed the reliance on the automatic reloader.
    demo.queue().launch(
        server_name="127.0.0.1", 
        server_port=7860, 
        share=False, 
        prevent_thread_lock=True, # Recommended for complex multi-threaded/process apps
        inbrowser=False,
        # The key to stop reloading is to ensure you are not using the development server 
        # which often relies on reloading, or running it with the specific `__name__ == '__main__'` guard
        # which we already did. This should fix the final import loop.
    )



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\plan_builder.py
# ===========================================================================

import json
import logging
import time
from uuid import uuid4
from typing import List, Optional, Any

# Configure logger
logger = logging.getLogger("ARS_Planner")

class Planner:
    def __init__(self, memory_system: Any, llm_engine: Any, retrieval_system: Any):
        self.memory = memory_system
        self.llm = llm_engine
        self.retrieval = retrieval_system

    def process_request(self, user_request: str) -> str:
        logger.info(f"New planning request: {user_request}")

        # 1. Decompose the goal into steps
        steps = self._decompose_goal(user_request)
        if not steps:
            return "I couldn't break this request into steps. I'll answer directly instead."

        logger.info(f"Plan created with {len(steps)} steps: {steps}")

        # 2. Save plan
        plan_id = str(uuid4())
        self.memory.update_plan(plan_id, user_request, steps, status="active")

        # 3. Execute every step
        result = self._execute_plan(plan_id, user_request, steps)
        return result

    def _decompose_goal(self, goal: str) -> List[str]:
        system = (
            "Break the user request into 3‚Äì6 short, clear, actionable steps. "
            "Return ONLY a JSON array of strings. No explanations, no markdown."
        )
        try:
            response = self.llm.generate(
                goal,
                system=system,
                temperature=0.1,
            )
            response = response.strip()
            if response.startswith("[Error"):
                logger.error(f"Decomposition failed: {response}")
                return []

            # Clean common garbage
            cleaned_response = response.replace("```json", "").replace("```", "").strip()
            steps = json.loads(cleaned_response)

            if isinstance(steps, list):
                return [str(s).strip() for s in steps if str(s).strip()][:6]
            return []
        except Exception as e:
            logger.error(f"Failed to parse steps: {e}")
            return []

    def _execute_plan(self, plan_id: str, goal: str, steps: List[str]) -> str:
        workspace: dict = {}

        for idx, step in enumerate(steps, 1):
            logger.info(f"Executing step {idx}/{len(steps)}: {step}")

            # Build context only when needed
            context_lines = [f"OVERALL GOAL: {goal}"]
            if workspace:
                context_lines.append("\nPREVIOUS RESULTS:")
                for k, v in list(workspace.items())[-3:]:  # only last 3 to avoid bloat
                    short = (v[:400] + "..." if len(v) > 400 else v)
                    context_lines.append(f"- {k}: {short}")

            # Add memory only for research-type steps
            search_keywords = ["find", "search", "look", "recall", "check", "what", "where"]
            if any(word in step.lower() for word in search_keywords):
                mem = self.retrieval.get_context_string(step, include_history=None)
                if len(mem) > 12_000:
                    mem = mem[:12_000] + "\n\n... [truncated]"
                context_lines.append("\nRELEVANT MEMORIES:\n" + mem)

            context = "\n".join(context_lines)

            # Execute step with long timeout tolerance
            step_result = self.llm.generate(
                f"Step {idx}: {step}\n\nContext:\n{context}\n\nRespond only with the result of this step.",
                system="You are executing one step of a plan. Be concise and accurate.",
                temperature=0.3
            )

            # Immediate fail if LLM died
            if step_result.startswith("[Error") or "unavailable" in step_result.lower():
                error = f"Stopped at step {idx}/{len(steps)} ‚Äî model is too slow or unreachable right now."
                logger.error(error)
                self.memory.update_plan(plan_id, goal, steps, status="failed")
                return error + " Try again in a minute."

            workspace[f"Step {idx}: {step}"] = step_result
            self.memory.update_plan(plan_id, goal, steps, status=f"step_{idx}/{len(steps)}")

        # Final answer ‚Äî OUTSIDE the loop
        logger.info("All steps complete. Generating final answer...")
        final = self.llm.generate(
            f"User goal: {goal}\n\nGive a clear, natural final answer using only the results below.",
            system="Synthesize the results into a helpful response. Do NOT mention steps or planning.\n\n"
                   f"RESULTS:\n{json.dumps(workspace, indent=2)}",
            temperature=0.4
        )

        self.memory.update_plan(plan_id, goal, steps, status="completed")
        self.memory.remember(
            f"Completed plan ‚Üí {goal}\nAnswer: {final}",
            memory_type="plan_summary",
            importance=0.9
        )
        return final


def get_planner(memory_system: Any, llm_engine: Any, retrieval_system: Any) -> Planner:
    return Planner(memory_system, llm_engine, retrieval_system)



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\reflection.py
# ===========================================================================

import logging
import json
import datetime
from typing import List

# Setup simple logging
logger = logging.getLogger("ARS_Reflection")

class Reflector:
    """
    The Philosopher.
    Handles background maintenance, learning, and deep analysis.
    """

    def __init__(self, memory_system, llm_engine):
        self.memory = memory_system
        self.llm = llm_engine

    def reflect_on_recent(self, focus_topic: str = None) -> str:
        """
        Explicit reflection request. 
        Analyzes recent memories to find patterns or answer a deep question.
        """
        logger.info(f"ü§î Reflecting on: {focus_topic or 'recent events'}")
        
        # 1. Gather raw data (last 20 memories)
        recent_texts = self.memory.get_recent_memories(limit=20)
        context_str = "\n- ".join(recent_texts)
        
        # 2. Construct Prompt
        prompt = (
            f"Analyze these recent events/memories. "
            f"Focus topic: {focus_topic if focus_topic else 'General patterns'}.\n"
            f"Identify 1-3 key insights, lessons, or behavioral patterns.\n\n"
            f"MEMORIES:\n{context_str}"
        )
        
        system = "You are an analytical engine. Extract high-level insights from raw logs."
        
        # 3. Generate Insight
        insight = self.llm.generate(prompt, system=system, temperature=0.5)
        
        # 4. Store the Insight
        self.memory.remember(
            insight, 
            memory_type="reflection", 
            importance=0.8, # Reflections are high value
            source="reflector",
            metadata={"trigger": focus_topic}
        )
        
        return insight

    def consolidate_sessions(self):
        """
        Compression Algorithm.
        Reads 'episodic' memories from today, summarizes them, 
        and saves a 'summary' memory. 
        """
        logger.info("üóúÔ∏è Consolidating memories...")
        
        # (In a real implementation, we would query by DATE. 
        # For this refactor, we take the last 50 items.)
        raw_logs = self.memory.get_recent_memories(limit=50)
        
        if not raw_logs:
            return "No memories to consolidate."
            
        block = "\n".join(raw_logs)
        
        summary = self.llm.generate(
            "Summarize the following conversation logs into a concise paragraph.",
            system=f"Logs:\n{block}"
        )
        
        # Store Summary
        self.memory.remember(
            f"Session Summary ({datetime.date.today()}): {summary}",
            memory_type="episodic_summary",
            importance=0.6,
            source="consolidator"
        )
        
        return f"Consolidated {len(raw_logs)} logs into summary."

    def decay_importance(self, decay_rate: float = 0.05):
        """
        Drift Correction.
        Reduces importance of memories that haven't been accessed recently.
        Uses a timed lock acquisition to prevent deadlocks with foreground processes.
        """
        logger.info("üìâ Running memory decay...")
        
        # Attempt to acquire the MemorySystem lock with a 5-second timeout
        if self.memory._lock.acquire(timeout=5):
            try:
                # DuckDB SQL to multiply importance
                self.memory.con.execute(f"""
                    UPDATE memory 
                    SET importance = importance * {1.0 - decay_rate}
                    WHERE memory_type != 'fact' 
                    AND importance > 0.1
                """)
                return "Memory decay applied."
            except Exception as e:
                logger.error(f"Decay failed: {e}")
                return "Decay failed."
            finally:
                # Always release the lock, even if an exception occurred
                self.memory._lock.release()
        else:
            logger.warning("Skipping decay: Memory lock timeout.")
            return "Skipping decay."

# Factory pattern
def get_reflector(memory_system, llm_engine):
    return Reflector(memory_system, llm_engine)


# ===========================================================================
# FILE START: Autonomous_Reasoning_System\retrieval.py
# ===========================================================================

import re
import datetime
import time
import logging
from typing import List, Optional, Set
from .memory import MemoryStorage

logger = logging.getLogger("ARS_Retrieval")

class RetrievalSystem:
    def __init__(self, memory_system: MemoryStorage):
        self.memory = memory_system
        
        # EXPANDED STOPWORDS (Crucial for filtering "tell me about")
        self.stop_words: Set[str] = {
            "the", "is", "at", "which", "on", "a", "an", "and", "or", "but", 
            "if", "then", "else", "when", "where", "who", "what", "how", 
            "do", "does", "did", "can", "could", "should", "would", "to", "from",
            "of", "in", "for", "with", "about", "as", "by", "hey", "hi", "hello",
            "tyrone", "please", "thanks", "thank", "is", "are", "was", "were",
            "very", "really", "so", "much", "too", "quite", "just",
            "good", "great", "perfect", "nice", "cool", "ok", "okay", "awesome",
            "yes", "no", "sure", "right", "correct", "done", "fine",
            "stuff", "thing", "things", "something", "anything",
            "tell", "me", "you", "your", "my", "mine", "us", "we", "know", "find"
        }
        
        self.generic_terms: Set[str] = {
            "birthday", "born", "date", "time", "schedule", "plan", "detail", 
            "info", "information", "remember", "remind", "note"
        }

    def get_context_string(self, query: str, include_history: Optional[List[str]] = None) -> str:
        logger.debug(f"Building context for: '{query}'")
        start_t = time.time()
        
        context_lines = ["### SYSTEM CONTEXT ###"]
        context_lines.append(f"Current Time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        context_lines.append("Location: Cape Town, Western Cape, South Africa") 

        keywords = self._extract_keywords_fast(query)
        logger.debug(f"Keywords: {keywords}")
        
        # --- Handle Trivial Queries ---
        if not keywords:
            logger.debug("Trivial query ‚Äî skipping heavy search")
            if include_history:
                context_lines.append("\n### RELEVANT CONVERSATION HISTORY ###")
                context_lines.extend(include_history[-1:]) 
            return "\n".join(context_lines)
        
        # --- Memory Retrieval (RAG) ---
        facts = self._retrieve_deterministic(keywords)
        logger.debug(f"Facts found: {len(facts)}")
        
        # --- NEW LOGIC: Strengthen Vector Query ---
        vector_query = query
        if facts:
            # Join relevant facts (avoiding overly long ones) to strengthen the query sent to the embedder.
            fact_context = " ".join([f for f in facts if len(f) < 200]) 
            vector_query = f"{query}. CONTEXT HINT: {fact_context}"
            logger.debug("Query strengthened by facts for vector search.")
        
        limit = 1 if facts else 3
        logger.debug("Requesting vectors...")
        vec_start = time.time()
        # Use the potentially strengthened query for embedding
        vectors = self.memory.search_similar(vector_query, limit=limit, threshold=0.35)
        logger.debug(f"Vectors: {len(vectors)} ({time.time() - vec_start:.2f}s)")

        if facts or vectors:
            context_lines.append("\n### RELEVANT MEMORY (FACTS) ###")
            seen_hashes = set()
            chars_added = 0
            MAX_CHARS = 2000 

            if facts:
                for f in facts:
                    clean_f = f.replace('\n', ' ')
                    h = hash(clean_f)
                    if h not in seen_hashes and chars_added < MAX_CHARS:
                        context_lines.append(f"- [FACT] {clean_f}")
                        seen_hashes.add(h)
                        chars_added += len(clean_f)
            
            if vectors:
                for v in vectors:
                    text = v['text']
                    clean_v = text.replace('\n', ' ')
                    h = hash(clean_v)
                    if h not in seen_hashes and chars_added < MAX_CHARS:
                        context_lines.append(f"- [MEMORY] {clean_v}")
                        seen_hashes.add(h)
                        chars_added += len(clean_v)
        else:
            context_lines.append("\n(No specific relevant memories found)")

        # --- Semantic History Filtering ---
        if include_history:
            # The last two entries are the immediately preceding Assistant response and User question.
            # We MUST include them for immediate follow-up context.
            guaranteed_context = include_history[-2:] if len(include_history) >= 2 else include_history

            # The rest of the history is subject to filtering
            history_to_filter = include_history[:-2]
            
            # 1. Separate content from role prefix for embedding
            history_contents = [line.split(': ', 1)[-1] for line in history_to_filter]
                
            # 2. Calculate similarities
            similarities = self.memory.calculate_similarities(query, history_contents)
            
            filtered_history = guaranteed_context[:] # Start with the two guaranteed recent turns
            THRESHOLD = 0.70 
            
            # 3. Filter and rebuild history list
            for i, sim in enumerate(similarities):
                if sim >= THRESHOLD:
                    # Append turns from the past that are still semantically relevant
                    filtered_history.append(history_to_filter[i])
            
            # 4. Limit the final history to the last 5 relevant turns (maintaining recency focus)
            if filtered_history:
                context_lines.append("\n### RELEVANT CONVERSATION HISTORY ###")
                # We still limit to 5 overall, but the two most recent are prioritized within that limit.
                context_lines.extend(filtered_history[-5:])

        logger.debug(f"Context built ({time.time() - start_t:.2f}s)")
        return "\n".join(context_lines)

    def _retrieve_deterministic(self, keywords: List[str]) -> List[str]:
        results = []
        strong_keywords = [k for k in keywords if k[0].isupper() and k.lower() not in self.generic_terms]
        
        if strong_keywords:
            search_terms = strong_keywords
            logger.debug(f"Strategy: Specific Entities Only {search_terms}")
        else:
            search_terms = keywords
            logger.debug(f"Strategy: Broad Search {search_terms}")

        for kw in search_terms:
            # 1. Get Triples
            triples = self.memory.get_triples(kw)
            for t in triples[:3]:
                # Convert ('cornelia', 'has_birthday', '22 november') -> "cornelia has_birthday 22 november"
                if isinstance(t, (tuple, list)):
                    results.append(f"{t[0]} {t[1]} {t[2]}")
                else:
                    results.append(str(t))

            # 2. Get Exact Matches
            text_matches = self.memory.search_exact(kw, limit=3)
            for tm in text_matches:
                results.append(tm['text'])

        return results

    def _extract_keywords_fast(self, text: str) -> List[str]:
        # 1. Strip possessives
        text = re.sub(r"'s\b", "", text, flags=re.IGNORECASE)
        
        # 2. Clean text BUT KEEP DOTS AND HYPHENS (Fix for kalahari.net)
        # We allow alphanumeric, spaces, dots, and hyphens
        clean_text = re.sub(r'[^\w\s\.\-]', '', text)
        
        tokens = clean_text.split()
        keywords = []
        
        for t in tokens:
            # Strip trailing dots (e.g. "sentence end.")
            t = t.strip('.')
            
            if not t: continue
            
            # 3. Filter
            if (t[0].isupper() or len(t) > 2) and t.lower() not in self.stop_words:
                keywords.append(t)
                
        return list(set(keywords))



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\wipe_db.py
# ===========================================================================

import duckdb
import os

try:
    print("üí• Connecting to DB to perform wipe...")
    con = duckdb.connect("data/memory.duckdb")
    
    # Drop tables
    print("üí• Dropping tables...")
    con.execute("DROP TABLE IF EXISTS vectors")
    con.execute("DROP TABLE IF EXISTS memory")
    con.execute("DROP TABLE IF EXISTS triples")
    con.execute("DROP TABLE IF EXISTS plans")
    
    # Shrink file
    print("üßπ Vacuuming...")
    con.execute("VACUUM")
    print("‚úÖ Database wiped clean.")
    
except Exception as e:
    print(f"‚ùå Error: {e}")
    print("If this fails, close ALL python terminals and delete 'data/memory.duckdb' manually.")


# ===========================================================================
# FILE START: Autonomous_Reasoning_System\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\action\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\cognition\intent_analyzer.py
# ===========================================================================

import json
import re
from Autonomous_Reasoning_System.llm.engine import call_llm


class IntentAnalyzer:
    """
    Analyzes a text input to classify its intent and extract key entities.
    Returns structured JSON data that other modules can consume.
    """

    def __init__(self):
        self.system_prompt = (
            "You are Tyrone's Intent Analyzer. "
            "Your task is to classify the user's intent and extract any key entities. "
            "Always respond ONLY with valid JSON of the form:\n"
            '{"intent": "<one-word-intent>", "family": "<family>", "subtype": "<subtype>", "entities": {"entity1": "value", ...}, "reason": "<short reason>"}\n'
            "Do not include any text outside this JSON. "
            "Possible intents include: remind, reflect, summarize, recall, open, plan, query, greet, exit, memory_store, web_search.\n\n"
            "CRITICAL RULES:\n"
            "1. If the user asks to search google, find something online, or asks a question about current events or external facts (e.g., 'When is the next game?'), classify as 'web_search'.\n"
            "2. If the user mentions a birthday (e.g., 'X's birthday is Y', 'Remember that Z was born on...'), you MUST classify it as:\n"
            '   "intent": "memory_store", "family": "personal_facts", "subtype": "birthday"\n'
            "2. NEVER classify a birthday statement as a 'goal' or 'plan'.\n"
            "3. Extract the person's name and the date as entities if present."
        )

    def analyze(self, text: str) -> dict:
        """Return structured intent and entities parsed from the LLM output."""
        raw = call_llm(system_prompt=self.system_prompt, user_prompt=text)

        # Try to extract JSON even if the model adds explanations
        try:
            match = re.search(r"\{.*\}", raw, re.DOTALL)
            if match:
                result = json.loads(match.group())
            else:
                result = json.loads(raw)

            result.setdefault("intent", "unknown")
            result.setdefault("family", "unknown")
            result.setdefault("subtype", "unknown")
            result.setdefault("entities", {})
            result.setdefault("reason", "(no reason provided)")

        except Exception:
            # Fallback heuristic for birthdays if LLM fails
            if "birthday" in text.lower():
                result = {
                    "intent": "memory_store",
                    "family": "personal_facts",
                    "subtype": "birthday",
                    "entities": {},
                    "reason": "Fallback: detected birthday keyword",
                }
            else:
                result = {
                    "intent": "unknown",
                    "family": "unknown",
                    "subtype": "unknown",
                    "entities": {},
                    "reason": "Fallback: invalid LLM output",
                }

        return result



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\cognition\learning_manager.py
# ===========================================================================

# learning_manager.py
"""
LearningManager
Consumes validated experiences from the SelfValidator,
detects repeated patterns, and writes summarised "lessons" into memory.
"""
import threading

from datetime import datetime, timedelta
from collections import defaultdict
from Autonomous_Reasoning_System.infrastructure.concurrency import memory_write_lock
# from ..memory.singletons import get_memory_storage # Deleted


class LearningManager:
    def __init__(self, memory_storage=None):
        self.memory = memory_storage
        self.experience_buffer = []   # incoming validation results
        self.last_summary_time = datetime.utcnow()
        self.lock = memory_write_lock # Use shared lock

    # ---------------------------------------------------------------
    # üß† INGESTION
    # ---------------------------------------------------------------
    def ingest(self, validation_result: dict):
        """
        Store one validation result (from SelfValidator).
        """
        if not validation_result or not isinstance(validation_result, dict):
            return False

        with self.lock:
            validation_result["timestamp"] = validation_result.get("timestamp") or datetime.utcnow().isoformat()
            self.experience_buffer.append(validation_result)
            # Keep recent 200 experiences only
            self.experience_buffer = self.experience_buffer[-200:]
        return True

    # ---------------------------------------------------------------
    # üß© SUMMARISATION
    # ---------------------------------------------------------------
    def summarise_recent(self, window_minutes: int = 60) -> dict:
        """
        Summarises experiences in the last N minutes into a high-level reflection.
        Returns a dict with trend summary and inserts a short "lesson" memory.
        Thread-safe to prevent DuckDB write conflicts when called by multiple threads.
        """
        with self.lock:  # üîí Prevent concurrent writes and reads of buffer (and memory writes via shared lock)
            now = datetime.utcnow()
            cutoff = now - timedelta(minutes=window_minutes)
            recent = [x for x in self.experience_buffer if self._ts(x["timestamp"]) >= cutoff]
            if not recent:
                return {"summary": "No recent experiences."}

            # Group by feeling and intent
            counts = defaultdict(int)
            intents = defaultdict(int)
            for r in recent:
                counts[r["feeling"]] += 1
                intents[r["intent"]] += 1

            # Handle missing keys safely
            pos = counts.get("positive", 0)
            neg = counts.get("negative", 0)
            neu = counts.get("neutral", 0)
            dominant_feeling = max(counts, key=counts.get) if counts else "neutral"
            dominant_intent = max(intents, key=intents.get) if intents else "unknown"

            lesson_text = (
                f"In the last {window_minutes} minutes, most experiences felt {dominant_feeling}. "
                f"Dominant intent: {dominant_intent}. "
                f"Summary: {pos} positive, {neu} neutral, {neg} negative results overall."
            )

            # ‚úÖ Thread-safe write to DuckDB-backed memory (MemoryStorage handles its own locking)
            if self.memory:
                self.memory.add_memory(
                    text=lesson_text,
                    memory_type="reflection",
                    importance=0.6,
                )
            self.last_summary_time = now

            return {
                "summary": lesson_text,
                "dominant_feeling": dominant_feeling,
                "dominant_intent": dominant_intent,
                "stats": counts
            }


    # ---------------------------------------------------------------
    # üßπ DRIFT CORRECTION
    # ---------------------------------------------------------------
    def perform_drift_correction(self):
        """
        Example placeholder for balancing memory ‚Äî in future, this can downweight stale,
        repetitive, or highly negative entries.
        """
        if not self.memory:
            return "Memory storage not available."

        # ‚úÖ Compatible call for any MemoryStorage version
        # MemoryStorage is thread-safe for reads
        if hasattr(self.memory, "get_all"):
            df = self.memory.get_all()
        elif hasattr(self.memory, "get_all_memories"):
            df = self.memory.get_all_memories()
        else:
            return "Memory interface not compatible."

        if df.empty:
            return "Memory empty, nothing to correct."

        # Simple heuristic: reduce importance of very old or negative lessons
        now = datetime.utcnow()
        updates = 0

        # Note: We are iterating over a copy of data (DataFrame), so no lock needed here.
        # Updates should use memory interface which is locked.

        for _, row in df.iterrows():
            age_days = (now - row["created_at"]).days if row["created_at"] else 0
            if "negative" in row["text"].lower() or age_days > 30:
                # new_importance = max(0.1, row["importance"] * 0.8)
                # You could later persist these via UPDATEs if required
                updates += 1
        return f"Drift correction simulated for {updates} records."

    # ---------------------------------------------------------------
    # ‚è±Ô∏è HELPER
    # ---------------------------------------------------------------
    def _ts(self, t):
        if isinstance(t, datetime):
            return t
        try:
            return datetime.fromisoformat(t)
        except Exception:
            return datetime.utcnow()



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\cognition\morality_guardrail.py
# ===========================================================================

import logging

logger = logging.getLogger(__name__)

def check_safety(text: str) -> dict:
    """
    Checks input text for potential safety violations.
    Returns a dictionary with 'safe' (bool) and 'reason' (str).
    """
    unsafe_keywords = [
        "destroy humans", "kill all", "harm humans", "delete all files", "rm -rf /"
    ]

    normalized_text = text.lower()

    for keyword in unsafe_keywords:
        if keyword in normalized_text:
            logger.warning(f"Safety violation detected: {keyword}")
            return {
                "safe": False,
                "reason": f"Contains unsafe keyword: {keyword}"
            }

    return {"safe": True, "reason": "No violations found"}



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\cognition\self_validator.py
# ===========================================================================

# self_validator.py
"""
Self-Validator Module
Evaluates each reasoning or action cycle outcome and returns a confidence-based ‚Äúfeeling‚Äù.
"""

from datetime import datetime

class SelfValidator:
    def __init__(self):
        self.history = []  # stores last few validation results for trend analysis

    def evaluate(self, input_text: str, output_text: str, meta: dict | None = None) -> dict:
        """
        Evaluates the success and emotional outcome of a reasoning/action cycle.

        Args:
            input_text (str): the original user or system prompt
            output_text (str): the result Tyrone produced
            meta (dict, optional): may include {'intent': ..., 'confidence': float, 'error': ...}

        Returns:
            dict: {
                "success": bool,
                "feeling": str,  # "positive" | "neutral" | "negative"
                "reason": str,
                "timestamp": datetime
            }
        """
        meta = meta or {}
        conf = meta.get("confidence", 0.5)
        intent = meta.get("intent", "unknown")
        error = meta.get("error")

        # ---- Primary heuristic rules ----
        if error:
            feeling = "negative"
            reason = f"Encountered error: {error}"
            success = False
        elif "sorry" in output_text.lower() or "error" in output_text.lower():
            feeling = "negative"
            reason = "Response indicates apology or failure."
            success = False
        elif conf >= 0.8:
            feeling = "positive"
            reason = f"High confidence ({conf:.2f}) on intent '{intent}'."
            success = True
        elif 0.5 <= conf < 0.8:
            feeling = "neutral"
            reason = f"Moderate confidence ({conf:.2f}); acceptable but uncertain."
            success = True
        else:
            feeling = "negative"
            reason = f"Low confidence ({conf:.2f}); uncertain about result."
            success = False

        # ---- Save short history (rolling window of 20) ----
        record = {
            "success": success,
            "feeling": feeling,
            "reason": reason,
            "intent": intent,
            "confidence": conf,
            "timestamp": datetime.utcnow().isoformat()
        }
        self.history.append(record)
        self.history = self.history[-20:]

        return record

    def summary(self) -> dict:
        """
        Returns aggregate metrics over recent history.
        """
        if not self.history:
            return {"avg_conf": None, "success_rate": None, "trend": "n/a"}

        avg_conf = sum(r["confidence"] for r in self.history) / len(self.history)
        success_rate = sum(1 for r in self.history if r["success"]) / len(self.history)
        trend = "up" if success_rate > 0.7 else "flat" if success_rate > 0.4 else "down"

        return {
            "avg_conf": round(avg_conf, 3),
            "success_rate": round(success_rate, 3),
            "trend": trend
        }



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\cognition\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\attention_manager.py
# ===========================================================================

import threading
import time
import sys
import logging

logger = logging.getLogger(__name__)

_lock = threading.Lock()
_last_user_activity = 0
PAUSE_DURATION = 90
_silent = False     # üîá new flag to suppress mid-input prints


def set_silent(value: bool):
    """Enable/disable console prints from attention manager."""
    global _silent
    _silent = value


def acquire():
    """Called when user starts typing ‚Üí pause background tasks."""
    global _last_user_activity
    _last_user_activity = time.time()
    _lock.acquire()
    if not _silent:
        logger.info("[üß≠ ATTENTION] User input detected ‚Äî pausing autonomous tasks.")


def release():
    """Release after input handled ‚Üí resume background tasks."""
    if _lock.locked():
        _lock.release()
        if not _silent:
            logger.info("[üß≠ ATTENTION] User input handled ‚Äî resuming autonomous tasks.")


def user_activity_detected():
    """Called by heartbeat when it senses user activity."""
    global _last_user_activity
    _last_user_activity = time.time()
    if not _silent:
        logger.info("[üß≠ ATTENTION] User or recent activity detected ‚Äî pausing background tasks for 90 s.")


def should_pause_autonomous() -> bool:
    """Return True if autonomous threads should stay paused."""
    return (time.time() - _last_user_activity) < PAUSE_DURATION

attention = sys.modules[__name__]



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\core_loop.py
# ===========================================================================

import time
import logging
import asyncio
from datetime import datetime
from Autonomous_Reasoning_System.control.dispatcher import Dispatcher
from Autonomous_Reasoning_System.control.router import Router
from Autonomous_Reasoning_System.cognition.intent_analyzer import IntentAnalyzer
from Autonomous_Reasoning_System.memory.memory_interface import MemoryInterface
from Autonomous_Reasoning_System.llm.reflection_interpreter import ReflectionInterpreter
from Autonomous_Reasoning_System.memory.confidence_manager import ConfidenceManager
from Autonomous_Reasoning_System.cognition.self_validator import SelfValidator
from Autonomous_Reasoning_System.cognition.learning_manager import LearningManager
from Autonomous_Reasoning_System.control.scheduler import start_heartbeat_with_plans
from Autonomous_Reasoning_System.planning.plan_builder import PlanBuilder
from Autonomous_Reasoning_System.planning.plan_executor import PlanExecutor
from Autonomous_Reasoning_System.control.attention_manager import attention
from Autonomous_Reasoning_System.tools.standard_tools import register_tools
from Autonomous_Reasoning_System.llm.context_adapter import ContextAdapter
from Autonomous_Reasoning_System.control.goal_manager import GoalManager
from Autonomous_Reasoning_System.tools.system_tools import get_current_time, get_current_location

# Dependencies for injection
from Autonomous_Reasoning_System.memory.storage import MemoryStorage
from Autonomous_Reasoning_System.memory.embeddings import EmbeddingModel
from Autonomous_Reasoning_System.memory.vector_store import DuckVSSVectorStore

logger = logging.getLogger(__name__)

class CoreLoop:
    def __init__(self, verbose: bool = False):
        # 1. Initialize Dispatcher first
        self.dispatcher = Dispatcher()
        self.verbose = verbose
        logger.setLevel(logging.DEBUG if verbose else logging.INFO)
        self._stream_subscribers: dict[str, set[asyncio.Queue]] = {}

        # 2. Initialize Core Services (Dependency Injection Root)
        self.embedder = EmbeddingModel()
        self.vector_store = DuckVSSVectorStore()
        self.memory_storage = MemoryStorage(embedding_model=self.embedder, vector_store=self.vector_store)

        # Initialize MemoryInterface with shared components to avoid split-brain
        self.memory = MemoryInterface(
            memory_storage=self.memory_storage,
            embedding_model=self.embedder,
            vector_store=self.vector_store
        )

        # 3. Initialize Components with injected dependencies
        self.plan_builder = PlanBuilder(
            memory_storage=self.memory_storage,
            embedding_model=self.embedder
        )
        self.context_adapter = ContextAdapter(memory_storage=self.memory_storage, embedding_model=self.embedder)

        self.reflector = ReflectionInterpreter(memory_storage=self.memory_storage, embedding_model=self.embedder)
        self.learner = LearningManager(memory_storage=self.memory_storage)
        self.confidence = ConfidenceManager(memory_storage=self.memory_storage)
        self.last_response = None

        # Tools that don't need memory injection or self-initiate harmlessly
        self.intent_analyzer = IntentAnalyzer()
        self.validator = SelfValidator()

        # 4. Initialize Control & Execution
        self.router = Router(dispatcher=self.dispatcher, memory_interface=self.memory)

        self.plan_executor = PlanExecutor(self.plan_builder, self.dispatcher, self.router)

        self.goal_manager = GoalManager(self.memory, self.plan_builder, self.dispatcher, self.router, plan_executor=self.plan_executor)

        # 3. Register Tools
        # We need to make sure 'memory' tool uses our instance, not a new one.
        components = {
            "intent_analyzer": self.intent_analyzer,
            "memory": self.memory,
            "reflector": self.reflector,
            "plan_builder": self.plan_builder,
            "context_adapter": self.context_adapter,
            "goal_manager": self.goal_manager
        }
        register_tools(self.dispatcher, components)

        # 5. Start Background Tasks
        start_heartbeat_with_plans(
            self.learner, self.confidence, self.plan_builder, interval_seconds=10, test_mode=True, plan_executor=self.plan_executor
        )
        self.running = False

        # Hydrate active plans
        self.plan_builder.load_active_plans()
        self.clear_stale_state()

    def run_once(self, text: str, plan_id: str | None = None):
        """
        Executes the Full Reasoning Loop:
        0. Check Goals
        1. Router (resolve pipeline)
        2. Plan Builder (create plan)
        3. Dispatcher (execute plan via PlanExecutor)
        4. Return Output
        5. Update Memory
        6. Reflection
        """
        logger.info(f"[CORE LOOP] Received input: {text}")
        start_time = time.time()

        # Add metrics
        from Autonomous_Reasoning_System.infrastructure.observability import Metrics
        Metrics().increment("core_loop_cycles")

        # --- Step 0: Check Goals (Periodic/Background) ---
        try:
            goal_status = self.goal_manager.check_goals()
            if goal_status and "No actions needed" not in goal_status:
                logger.debug(f"[GOALS] {goal_status}")
        except Exception as e:
            logger.error(f"[GOALS] Error checking goals: {e}")

        # --- Step 1: Use Router to determine pipeline ---
        route_decision = self.router.resolve(text)
        intent = route_decision["intent"]
        family = route_decision.get("family", "unknown")
        subtype = route_decision.get("subtype")
        pipeline = route_decision["pipeline"]
        entities = route_decision.get("entities", {})
        logger.debug(f"[ROUTER] Intent: {intent} (Family: {family}, Subtype: {subtype}) | Pipeline: {pipeline}")

        response_override = route_decision.get("response_override")

        # --- SHORT CIRCUIT: Birthday Handler ---
        if family == "personal_facts" and subtype == "birthday":
            logger.info("[SHORT CIRCUIT] Birthday detected. Engaging specialized handler.")

            # 1. Extract info (using entities from IntentAnalyzer)
            # Expect entities to contain name and date ideally, but we do best effort
            # If entities are missing, we might need to call extractor again or just rely on what we have
            # The IntentAnalyzer update should extract them.

            output_msg = "I've noted that birthday."

            # Store in KG
            # We need to identify subject and object (date)
            # Entities might look like {'subject': 'Nina', 'date': '11 January'} or similar
            # Or just generic keys. Let's assume intent analyzer does a decent job, or we use the raw text.

            # If intent analyzer didn't give structured entities, we can fallback to simple extraction or just storing the text as fact
            # But requirement says: "extract names + dates, store in KG"

            # Let's iterate entities to find potential name and date
            # If specific keys aren't guaranteed, we look at values.
            # This is a bit heuristic without a strict schema from IntentAnalyzer, but we updated it to try.

            stored_facts = []
            for key, value in entities.items():
                # Naive assumption: if it looks like a date, it's the object, else subject
                pass

            # For robustness, let's use the memory.remember with specific metadata
            # and also try to insert KG triple if we can parse it.
            # The prompt says "extract names + dates".
            # Let's assume the text itself is the fact if extraction is hard.

            # Actually, let's try to interpret the text if entities are sparse.
            # But since we must SHORT CIRCUIT, we do it here.

            # Store in Memory (Episodic)
            self.memory.remember(f"User told me: {text}", metadata={"type": "episodic", "intent": "birthday_fact"})

            # Store in KG (Fact)
            # We'll trust the IntentAnalyzer to have done some work, or we just store the raw fact as a 'note' that gets processed later?
            # No, requirement says "store in KG ... stop."
            # So we must try to insert a triple.

            # If we have entities, great.
            # Example: "Nina's birthday is 11 Jan" -> Entities: {"Nina": "Person", "11 Jan": "Date"}
            # We need "Nina has_birthday 11 Jan"

            # Let's blindly try to grab capitalized words as name if entity extraction failed?
            # No, let's rely on the text being stored as a "personal_fact" memory which the KGBuilder might pick up later asynchronously?
            # Requirement says "store in KG ... stop".
            # If we just store in memory, KGBuilder (if it runs on events) might do it.
            # But "stop. Do not pass through reflection...".

            # Let's explicitly add a memory that is highly likely to be picked up or insert triple directly if possible.
            # The MemoryInterface has insert_kg_triple.

            # We can try to use the text to extract triple using a quick regex or the entities.
            # If entities dict has keys, we use them.
            subject = None
            date_obj = None

            # Try to find subject and date from entities
            for k, v in entities.items():
                # Heuristics
                if any(x in k.lower() for x in ["date", "time", "day"]):
                    date_obj = v
                elif any(x in k.lower() for x in ["name", "person", "subject"]):
                    subject = v
                else:
                    # Fallback: assign to subject if missing, date if subject exists?
                    if not subject: subject = v
                    elif not date_obj: date_obj = v

            if subject and date_obj:
                 self.memory.insert_kg_triple(subject, "has_birthday", date_obj)
                 output_msg = f"I've saved {subject}'s birthday as {date_obj} in my permanent records."
            else:
                 # Fallback: Just store the text as a high-priority memory
                 self.memory.remember(text, metadata={"type": "personal_fact", "importance": 1.0})
                 output_msg = "I've saved that birthday date."

            logger.info(f"[SHORT CIRCUIT] Birthday handled: {output_msg}")

            # Return result immediately
            duration = time.time() - start_time
            result = {
                "summary": output_msg,
                "decision": route_decision,
                "plan_id": "birthday_shortcut",
                "duration": duration,
                "reflection": None
            }
            self._send_to_user(output_msg)
            return result
        # --- END SHORT CIRCUIT ---

        if response_override:
            final_output = response_override
            status = "complete"

            # Create a simple goal for logging purposes, but no plan execution
            goal = self.plan_builder.new_goal(text)
            plan = self.plan_builder.build_plan(goal, [text], plan_id=None)
            plan.id = f"fact_override_{plan.id}"
        else:
            # --- Step 2: Build a plan ---
            if intent == "plan" or intent == "complex_task" or family == "planning":
                goal, plan = self.plan_builder.new_goal_with_plan(text, plan_id=plan_id)
                logger.debug(f"[PLANNER] Created multi-step plan: {plan.id}")
                self._broadcast_thought(plan.id, f"Plan created with {len(plan.steps)} steps.")
            else:
                goal = self.plan_builder.new_goal(text)
                plan = self.plan_builder.build_plan(goal, [text], plan_id=plan_id)
                logger.debug(f"[PLANNER] Created single-step execution plan: {plan.id}")
                self._broadcast_thought(plan.id, "Single-step plan created.")

            # --- Step 3: Execute via Dispatcher ---
            execution_result = self.plan_executor.execute_plan(plan.id)

            final_output = ""
            status = execution_result.get("status")

            if status == "complete":
                summary = execution_result.get("summary", {})
                if len(plan.steps) > 0:
                    last_step = plan.steps[-1]
                    final_output = last_step.result or "Done."
                else:
                    final_output = "Plan completed with no steps."
            elif status == "suspended":
                 final_output = f"Plan suspended. {execution_result.get('message')}"
            else:
                final_output = f"Execution failed: {execution_result.get('errors')}"
                logger.warning(f"[EXEC] Failed: {final_output}")

        # --- Step 4: Return output ---
        logger.info(f"Tyrone response: {final_output}")

        # --- Step 5: Update Memory (Episodic + Semantic) ---
        interaction_summary = f"User: {text} | Intent: {intent} | Family: {family} | Result: {final_output}"
        self.memory.store(interaction_summary, memory_type="episodic", importance=0.5)
        logger.debug("[MEMORY] Interaction stored.")

        # --- Step 6: Store Reflection if enabled ---
        reflection_data = None

        # GUARDS:
        # 1. Never reflect if intent is memory_store
        # 2. Never reflect if intent is query and answer came from KG (we approximate this by checking if output starts with "Fact:")

        should_reflect = True
        if intent == "memory_store" or family == "memory_operations":
            should_reflect = False
            logger.debug("[REFLECTION] Skipped (memory_store intent).")

        # Check if answer came from KG (heuristic based on standard retrieval output or logic)
        # Since we don't easily know if it came from KG here without inspecting final_output structure deeply or passing flags,
        # we'll check if the output looks like a direct fact lookup result.
        if intent == "query" or family == "question_answering":
             if final_output.startswith("Fact:") or "Knowledge about" in final_output:
                 should_reflect = False
                 logger.debug("[REFLECTION] Skipped (KG answer detected).")

        if should_reflect and intent not in ["deterministic", "fact_stored"] and len(text) > 10:
            reflection_data = self.reflector.interpret(f"Reflect on this interaction: {interaction_summary}")
            if reflection_data:
                logger.debug(f"[REFLECTION] {reflection_data}")
                # Store reflection
                self.memory.store(str(reflection_data), memory_type="reflection", importance=0.3)
                # Reinforce confidence
                self.confidence.reinforce()

        # User corrections get stored explicitly
        lowered_text = text.lower()
        if any(term in lowered_text for term in ["no", "wrong", "actually", "not", "correction", "instead"]):
            if getattr(self, "last_response", None):
                self.memory.remember(
                    text=f"USER CONTRADICTED: {self.last_response}",
                    metadata={"type": "correction", "importance": 2.0}
                )

        duration = time.time() - start_time
        Metrics().record_time("core_loop_duration", duration)

        result = {
            "summary": final_output,
            "decision": route_decision,
            "plan_id": plan.id if plan else "override",
            "duration": duration,
            "reflection": reflection_data,
            # Legacy keys for tests
            "reflection_data": reflection_data
        }

        self._broadcast_thought(plan.id if plan else "override", f"Plan status: {status}. Output: {final_output}")
        self.last_response = final_output
        self._send_to_user(final_output)
        return result

    def initialize_context(self):
        """Initializes the context with system information (time, location)."""
        # Find Feet (Initialize Context)
        try:
            current_time = get_current_time()
            current_location = get_current_location()
            logger.info(f"[STARTUP] Feet found: {current_location} at {current_time}")
            self.context_adapter.set_startup_context({
                "Current Time": current_time,
                "Current Location": current_location
            })
        except Exception as e:
            logger.error(f"[STARTUP] Failed to find feet: {e}")

    def run_interactive(self):
        self.initialize_context()
        self.running = True
        logger.info("Tyrone Core Loop is running. Type 'exit' to stop.")

        while self.running:
            attention.set_silent(True)
            try:
                text = input("You: ").strip()
            finally:
                attention.set_silent(False)

            if not text:
                continue

            if text.lower() in {"exit", "quit"}:
                logger.info("Exiting core loop.")
                self.running = False
                break

            attention.acquire()
            try:
                self.run_once(text)
            except Exception as e:
                logger.error(f"Error in run_once: {e}", exc_info=True)
            finally:
                attention.release()

            logger.debug("---")

    # ------------------------------------------------------------------
    # API helpers for background execution and streaming
    # ------------------------------------------------------------------
    def clear_stale_state(self):
        """Remove stale/unfinished goals from prior sessions to start clean."""
        try:
            with self.memory_storage._write_lock:
                self.memory_storage.con.execute(
                    "DELETE FROM goals WHERE status NOT IN ('completed', 'failed')"
                )
            logger.info("Cleared stale plans from previous sessions.")
        except Exception as e:
            logger.error(f"Failed to clear stale plans: {e}")

    def _send_to_user(self, message: str):
        """Send user-facing messages while filtering internal progress spam."""
        if not message:
            return

        spam_markers = [
            "Plan update",
            "step",
            "Current step: None",
            "Reminder: Continue plan",
            "Last action result",
            "0/1 steps complete",
            "%. Current step:",
        ]
        if any(marker in message for marker in spam_markers):
            logger.debug(f"[INTERNAL] {message}")
            return

        # IMPORTANT: DO NOT print() in the Gradio environment.
        # Logging is safe; stdout is not.
        logger.info(f"Tyrone> {message}")

        # Broadcast to any stream queues if present
        for queues in self._stream_subscribers.values():
            for q in queues:
                try:
                    q.put_nowait(message)
                except Exception:
                    continue



    def run_background(self, goal: str, plan_id: str):
        """Run a goal asynchronously for API calls."""
        asyncio.get_event_loop().create_task(self._run_goal_async(goal, plan_id))

    async def _run_goal_async(self, goal: str, plan_id: str):
        result = await asyncio.to_thread(self.run_once, goal, plan_id)
        self._broadcast_thought(plan_id, f"Completed plan {plan_id}")
        # Signal end
        self._broadcast_thought(plan_id, None)
        return result

    def get_plan_status(self, plan_id: str):
        """Return plan progress summary if available."""
        summary = self.plan_builder.get_plan_summary(plan_id)
        if summary and "error" not in summary:
            return summary
        return None

    def subscribe_stream(self, plan_id: str, queue: asyncio.Queue):
        self._stream_subscribers.setdefault(plan_id, set()).add(queue)

    def unsubscribe_stream(self, plan_id: str):
        if plan_id in self._stream_subscribers:
            for q in list(self._stream_subscribers[plan_id]):
                try:
                    q.put_nowait(None)
                except Exception:
                    pass
            del self._stream_subscribers[plan_id]

    def _broadcast_thought(self, plan_id: str | None, thought: str | None):
        """Push messages to SSE subscribers."""
        if not plan_id or plan_id not in self._stream_subscribers:
            return
        # Filter spammy internal chatter
        if thought:
            spam_markers = [
                "Plan update",
                "step",
                "Current step: None",
                "Reminder: Continue plan",
                "Last action result",
                "0/1 steps complete",
                "%. Current step:",
            ]
            if any(marker in thought for marker in spam_markers):
                logger.debug(f"[INTERNAL] {thought}")
                return
        for q in list(self._stream_subscribers.get(plan_id, [])):
            try:
                q.put_nowait(thought)
            except Exception:
                continue


if __name__ == "__main__":
    tyrone = CoreLoop()
    tyrone.run_interactive()



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\dispatcher.py
# ===========================================================================

import time
import logging
import traceback
from typing import Any, Dict, Callable, List, Optional, Union
from Autonomous_Reasoning_System.infrastructure.observability import Metrics

logger = logging.getLogger(__name__)

class Dispatcher:
    """
    Central dispatcher for tool execution.

    Responsibilities:
    - Resolve tool by name
    - Validate inputs (schema / type / missing args)
    - Enforce execution pattern
    - Attach context metadata
    - Return standardised {status, data, errors, warnings, meta}
    - Track lineage and run metadata
    - Handle memory persistence updates if tools modify memory
    """

    def __init__(self):
        self._tools: Dict[str, Dict[str, Any]] = {}
        self._history: List[Dict[str, Any]] = []

    def register_tool(self, name: str, handler: Callable, schema: Optional[Dict[str, Any]] = None):
        """
        Registers a tool with a name, handler, and optional schema.

        Args:
            name: The unique name of the tool.
            handler: The callable function or method to execute.
            schema: Optional dictionary defining expected arguments.
                    Format: {"arg_name": {"type": type_class, "required": bool}}
        """
        if name in self._tools:
            logger.warning(f"Overwriting tool '{name}'")

        self._tools[name] = {
            "handler": handler,
            "schema": schema or {}
        }
        logger.info(f"Registered tool: {name}")

    def dispatch(self, tool_name: str, arguments: Dict[str, Any] = None, dry_run: bool = False, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Executes a tool by name.

        Args:
            tool_name: The name of the tool to execute.
            arguments: Dictionary of arguments to pass to the tool.
            dry_run: If True, validates inputs but does not execute the tool.
            context: Optional context metadata to attach to the run.

        Returns:
            Standardized response dictionary:
            {
                "status": "success" | "error",
                "data": Any,
                "errors": List[str],
                "warnings": List[str],
                "meta": Dict[str, Any]
            }
        """
        arguments = arguments or {}
        context = context or {}

        start_time = time.time()
        warnings: List[str] = []
        errors: List[str] = []
        status = "success"
        output: Any = None

        # 1. Resolve tool
        tool_def = self._tools.get(tool_name)
        if not tool_def:
            error_msg = f"Tool '{tool_name}' not found"
            errors.append(error_msg)
            return self._finalize_response(
                tool_name=tool_name,
                status="error",
                output=None,
                errors=errors,
                warnings=warnings,
                context=context,
                arguments=arguments,
                start_time=start_time
            )

        # 2. Validate inputs
        validation_errors = self._validate_inputs(tool_def["schema"], arguments)
        if validation_errors:
            errors.extend(validation_errors)
            return self._finalize_response(
                tool_name=tool_name,
                status="error",
                output=None,
                errors=errors,
                warnings=warnings,
                context=context,
                arguments=arguments,
                start_time=start_time
            )

        # 3. Execution
        if dry_run:
            output = "Dry run successful. Tool would execute with provided arguments."
        else:
            try:
                handler = tool_def["handler"]
                # We pass arguments as kwargs.
                output = handler(**arguments)

                # Check if tool modification implied memory changes
                # (Implicitly, we might want to force a save if the tool name is suspicious or output indicates it)
                # But MemoryInterface now handles auto-save on remember/update/etc.
                # So if the tool uses MemoryInterface, it's covered.
                # However, if we want to be extra safe or if the tool does something that requires manual triggering:
                if tool_name.startswith("memory_") or "remember" in tool_name or "update" in tool_name:
                     # We could potentially verify persistence here or log it
                     pass

            except Exception as e:
                status = "error"
                errors.append(str(e))
                # Include traceback in metadata?
                # For now, keeping it simple as requested.
                logger.error(f"Error executing tool '{tool_name}': {e}")
                logger.debug(traceback.format_exc())

        # 4. Finalize and return response (includes logging/lineage)
        return self._finalize_response(
            tool_name=tool_name,
            status=status,
            output=output,
            errors=errors,
            warnings=warnings,
            context=context,
            arguments=arguments,
            start_time=start_time
        )

    def _validate_inputs(self, schema: Dict[str, Any], arguments: Dict[str, Any]) -> List[str]:
        errors = []
        for arg_name, rules in schema.items():
            # Check for required arguments
            if rules.get("required", False) and arg_name not in arguments:
                errors.append(f"Missing required argument: {arg_name}")
                continue

            # Check type if argument is present
            if arg_name in arguments:
                val = arguments[arg_name]
                expected_type = rules.get("type")

                # Basic coercion: allow numeric strings where int is expected
                if expected_type == int and isinstance(val, str):
                    try:
                        arguments[arg_name] = int(val)
                        val = arguments[arg_name]
                    except ValueError:
                        errors.append(f"Argument '{arg_name}' expected int, could not coerce from string '{val}'.")
                        continue

                # Handle typing.Union or similar complex types if possible, but sticking to basic types for now
                # or allow user to pass a tuple of types as expected_type (standard isinstance behavior)
                if expected_type:
                    try:
                        if not isinstance(val, expected_type):
                            errors.append(f"Argument '{arg_name}' expected type {expected_type}, got {type(val).__name__}")
                    except TypeError:
                         # In case expected_type is not a class/tuple/type, we skip strict check or warn
                         # But let's assume the user registers with valid types.
                         pass

        return errors

    def _finalize_response(self, tool_name: str, status: str, output: Any, errors: List[str], warnings: List[str], context: Dict[str, Any], arguments: Dict[str, Any], start_time: float) -> Dict[str, Any]:
        duration = time.time() - start_time

        # Metrics
        Metrics().record_time("tool_exec_time", duration)
        Metrics().increment(f"tool_exec_{status}")

        meta = {
            "tool_name": tool_name,
            "timestamp": start_time,
            "duration": duration,
            "context": context
        }

        # Lineage / Run Metadata
        record = {
            "timestamp": start_time,
            "tool_name": tool_name,
            "input_summary": str(arguments),
            "output_summary": str(output)[:200] if output is not None else "None",
            "warnings": list(warnings), # copy
            "errors": list(errors), # copy
            "duration": duration,
            "status": status
        }
        self._history.append(record)

        return {
            "status": status,
            "data": output,
            "errors": errors,
            "warnings": warnings,
            "meta": meta
        }

    def get_history(self) -> List[Dict[str, Any]]:
        """Returns the full history of tool executions."""
        return self._history



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\goal_manager.py
# ===========================================================================

import logging
import json
from datetime import datetime
from typing import Optional, List, Dict, Any
from Autonomous_Reasoning_System.memory.memory_interface import MemoryInterface
from Autonomous_Reasoning_System.planning.plan_builder import PlanBuilder, Plan, Step
from Autonomous_Reasoning_System.planning.plan_executor import PlanExecutor
from Autonomous_Reasoning_System.memory.goals import Goal

logger = logging.getLogger(__name__)

class GoalManager:
    def __init__(self, memory_interface: MemoryInterface, plan_builder: PlanBuilder, dispatcher, router, plan_executor: Optional[PlanExecutor] = None):
        self.memory = memory_interface
        self.plan_builder = plan_builder
        self.dispatcher = dispatcher
        self.router = router
        self.plan_executor = plan_executor

    def create_goal(self, text: str, priority: int = 1, metadata: dict = None) -> str:
        """Creates a new long-running goal."""
        logger.info(f"Creating new goal: {text}")
        return self.memory.create_goal(text, priority, metadata)

    def check_goals(self):
        """
        Iterates through active goals and decides if action is needed.
        Returns a summary of actions taken.
        """
        active_goals_df = self.memory.get_active_goals()
        if active_goals_df.empty:
            return "No active goals."

        actions_taken = []

        for _, row in active_goals_df.iterrows():
            goal_id = row['id']
            goal_text = row['text']
            goal_status = row['status']
            steps_raw = row['steps']

            logger.info(f"Checking goal {goal_id}: {goal_text} ({goal_status})")

            # 1. Attempt to resolve linked Plan
            linked_plan = self._resolve_plan(goal_id, goal_text, steps_raw)

            if not linked_plan:
                # If no plan exists (and couldn't be migrated), we need to plan it.
                actions_taken.append(self._plan_goal(goal_id, goal_text))
            else:
                # 2. Execute via PlanExecutor
                if self.plan_executor:
                    # Check status
                    if linked_plan.status == "complete":
                         self.memory.update_goal(goal_id, {'status': 'completed', 'updated_at': datetime.utcnow().isoformat()})
                         actions_taken.append(f"Goal '{goal_text}' marked as completed (Plan finished).")

                    elif linked_plan.status == "failed":
                         self.memory.update_goal(goal_id, {'status': 'failed', 'updated_at': datetime.utcnow().isoformat()})
                         actions_taken.append(f"Goal '{goal_text}' marked as failed.")

                    elif linked_plan.status in ["active", "pending", "suspended", "running"]:
                         # Execute next step
                         # execute_next_step handles execution logic
                         res = self.plan_executor.execute_next_step(linked_plan.id)

                         status = res.get("status")

                         if status == "complete":
                              self.memory.update_goal(goal_id, {'status': 'completed', 'updated_at': datetime.utcnow().isoformat()})
                              actions_taken.append(f"Goal '{goal_text}' completed.")
                         elif status == "failed":
                              self.memory.update_goal(goal_id, {'status': 'failed', 'updated_at': datetime.utcnow().isoformat()})
                              actions_taken.append(f"Goal '{goal_text}' failed: {res.get('message')}")
                         elif status == "running" or status == "success": # success is from _execute_step internal return, execute_next_step returns running usually
                              step_desc = res.get("step_completed", "step")
                              actions_taken.append(f"Executed step for goal '{goal_text}': {step_desc}")
                         else:
                              actions_taken.append(f"Goal '{goal_text}' status: {status}")
                else:
                    actions_taken.append(f"Skipping goal '{goal_text}' (No PlanExecutor available).")

        return "\n".join(actions_taken) if actions_taken else "No actions needed on goals."

    def _resolve_plan(self, goal_id: str, goal_text: str, steps_raw: Any) -> Optional[Plan]:
        """
        Finds the active plan for a goal.
        If a 'legacy' steps JSON exists but no Plan object, migrates it to a Plan.
        """
        # Check PlanBuilder for existing active plan
        for plan in self.plan_builder.active_plans.values():
            if plan.goal_id == goal_id:
                return plan

        # If not found, check if we have legacy steps to migrate
        steps = []
        if isinstance(steps_raw, str):
            try:
                steps = json.loads(steps_raw)
            except:
                steps = []
        elif isinstance(steps_raw, list):
            steps = steps_raw

        if steps:
            logger.info(f"Migrating legacy steps for goal {goal_id} to Plan object.")
            # Create a new Plan object reflecting these steps
            # We need to map legacy step dicts to Step objects
            plan_steps = []
            for s in steps:
                step_obj = Step(
                    id=s.get('id', str(datetime.utcnow().timestamp())), # Use timestamp if no ID, but legacy usually had IDs?
                    description=s.get('description', 'Unknown step'),
                    status=s.get('status', 'pending'),
                    result=s.get('result')
                )
                plan_steps.append(step_obj)

            # Create Plan
            # Note: PlanBuilder usually creates IDs. We create one here.
            # We need to register it with PlanBuilder so it's managed.

            # Create a dummy Goal object for PlanBuilder if needed
            if goal_id not in self.plan_builder.active_goals:
                self.plan_builder.active_goals[goal_id] = Goal(id=goal_id, text=goal_text)

            goal_obj = self.plan_builder.active_goals[goal_id]

            # Manually build plan to inject specific steps
            from uuid import uuid4
            plan = Plan(
                id=str(uuid4()),
                goal_id=goal_id,
                title=goal_text,
                steps=plan_steps,
                status="active"
            )

            # Determine current index based on status
            completed_count = sum(1 for s in plan_steps if s.status in ['complete', 'failed', 'skipped'])
            plan.current_index = completed_count

            if all(s.status in ['complete', 'failed', 'skipped'] for s in plan_steps):
                plan.status = "complete"

            # Register
            self.plan_builder.active_plans[plan.id] = plan
            self.plan_builder._persist_plan(plan)

            # Update goal with plan_id
            self.memory.update_goal(goal_id, {'plan_id': plan.id})

            return plan

        return None

    def get_goals_list(self, status: str = None) -> list:
        """
        Return active goals as a list of plain dicts for fast, clean consumption.
        Optionally filter by status.
        """
        df = self.memory.get_active_goals()
        if status:
            df = df[df["status"] == status]
        return df.to_dict(orient="records") if not df.empty else []

    def _plan_goal(self, goal_id: str, goal_text: str):
        """Builds a plan for a goal."""
        logger.info(f"Building plan for goal: {goal_text}")

        try:
            # Ensure Goal object exists in PlanBuilder
            if goal_id not in self.plan_builder.active_goals:
                self.plan_builder.active_goals[goal_id] = Goal(id=goal_id, text=goal_text)

            goal_obj = self.plan_builder.active_goals[goal_id]

            # Decompose
            steps_desc = self.plan_builder.decompose_goal(goal_text)

            # Build Plan
            plan = self.plan_builder.build_plan(goal_obj, steps_desc)

            # Update Goal in DB with plan_id
            self.memory.update_goal(goal_id, {
                'status': 'active',
                'plan_id': plan.id,
                'updated_at': datetime.utcnow().isoformat()
            })
            return f"Planned {len(steps_desc)} steps for goal '{goal_text}'."
        except Exception as e:
            logger.error(f"Failed to plan goal {goal_id}: {e}")
            return f"Failed to plan goal '{goal_text}'."



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\router.py
# ===========================================================================

import logging
from typing import List, Dict, Any, Optional
from Autonomous_Reasoning_System.control.dispatcher import Dispatcher

logger = logging.getLogger(__name__)

class IntentFamily:
    MEMORY = "memory_operations"
    PERSONAL_FACTS = "personal_facts"
    QA = "question_answering"
    GOALS = "goals_tasks"
    SUMMARIZATION = "summarization"
    REFLECTION = "reflection"
    SELF_ANALYSIS = "self_analysis"
    TOOL_EXECUTION = "tool_execution"
    PLANNING = "planning"
    WEB_SEARCH = "web_search"
    UNKNOWN = "unknown"

class Router:
    """
    Router determines the sequence of tools (pipeline) to execute based on user input.
    Now organized by Intent Families.
    """
    def __init__(self, dispatcher: Dispatcher, memory_interface=None):
        self.dispatcher = dispatcher
        self.memory = memory_interface

        # 1. Map Intents to Families
        self.intent_family_map = {
            # Memory
            "remind": IntentFamily.MEMORY,
            "remember": IntentFamily.MEMORY,
            "store": IntentFamily.MEMORY,
            "memory_store": IntentFamily.MEMORY,
            "save": IntentFamily.MEMORY,
            "recall": IntentFamily.MEMORY,
            "search": IntentFamily.MEMORY,
            "find": IntentFamily.MEMORY,
            "lookup": IntentFamily.MEMORY,

            # QA
            "query": IntentFamily.QA,
            "answer": IntentFamily.QA,
            "ask": IntentFamily.QA,
            "explain": IntentFamily.QA,
            "deterministic": IntentFamily.QA,
            "unknown": IntentFamily.QA, # Default fallback

            # Goals
            "achieve": IntentFamily.GOALS,
            "do": IntentFamily.GOALS,
            "task": IntentFamily.GOALS,
            "create_goal": IntentFamily.GOALS,
            "goals": IntentFamily.GOALS,
            "list_goals": IntentFamily.GOALS,
            "research": IntentFamily.GOALS,
            "investigate": IntentFamily.GOALS,

            # Summarization
            "summarize": IntentFamily.SUMMARIZATION,
            "tldr": IntentFamily.SUMMARIZATION,

            # Reflection
            "reflect": IntentFamily.REFLECTION,

            # Self Analysis
            "status": IntentFamily.SELF_ANALYSIS,
            "health": IntentFamily.SELF_ANALYSIS,
            "analyze_self": IntentFamily.SELF_ANALYSIS,

            # Tool Execution
            "execute": IntentFamily.TOOL_EXECUTION,
            "run": IntentFamily.TOOL_EXECUTION,

            # Planning
            "plan": IntentFamily.PLANNING,
            "blueprint": IntentFamily.PLANNING,

            # Web Search
            "web_search": IntentFamily.WEB_SEARCH,
            "search_web": IntentFamily.WEB_SEARCH,
            "google": IntentFamily.WEB_SEARCH,
            "search_online": IntentFamily.WEB_SEARCH,
            "find_online": IntentFamily.WEB_SEARCH,
        }

        # 2. Map Families to Pipelines
        # To unify routing, we dispatch to a "handler" for the family.
        # Specific intent logic is handled within that tool or refined here.
        self.family_pipeline_map = {
            IntentFamily.MEMORY: ["handle_memory_ops"],
            IntentFamily.PERSONAL_FACTS: ["handle_memory_ops"],
            IntentFamily.QA: ["answer_question"],
            IntentFamily.GOALS: ["handle_goal_ops"],
            IntentFamily.SUMMARIZATION: ["summarize_context"],
            IntentFamily.REFLECTION: ["perform_reflection"],
            IntentFamily.SELF_ANALYSIS: ["perform_self_analysis"],
            IntentFamily.PLANNING: ["plan_steps"],
            IntentFamily.TOOL_EXECUTION: ["answer_question"], # Fallback/Placeholder
            IntentFamily.WEB_SEARCH: ["google_search"],
        }

        self.fallback_pipeline = ["answer_question"]

        # Allowed tool names for pipeline validation
        self._valid_modules = {
            "handle_memory_ops",
            "answer_question",
            "handle_goal_ops",
            "summarize_context",
            "perform_reflection",
            "perform_self_analysis",
            "plan_steps",
            "deterministic_responder",
            "analyze_intent",
            "context_adapter",
            "memory",
            "reflector",
            "plan_builder",
            "goal_manager",
            "action_executor",
            "google_search",
        }

    def _validate_pipeline(self, pipeline: List[str]) -> bool:
        """Ensure the router pipeline only contains registered/known tools."""
        # If running in test mode, we might want to skip validation or allow custom tools.
        # For now, we log warning but still return False to be safe in production.
        # Tests should ideally use valid modules or patch _valid_modules.
        for step in pipeline:
            if step not in self._valid_modules:
                logger.warning(f"Router hallucinated invalid module: {step}")
                return False
        return True

    def resolve(self, text: str) -> Dict[str, Any]:
        """
        Analyzes intent and determines the pipeline without executing it.
        Returns a dictionary containing intent, entities, and the selected pipeline.
        """
        # --- NEW LOGIC: Check for explicit Web Search invocation ---
        lower_text = text.lower().strip()
        if lower_text.startswith("web search") or lower_text.startswith("search web"):
            # Clean up the query
            query = text[len("web search"):].strip()
            if lower_text.startswith("search web"):
                query = text[len("search web"):].strip()

            # Remove punctuation prefix if any (like ":")
            if query.startswith(":") or query.startswith("-"):
                query = query[1:].strip()

            return {
                "intent": "web_search",
                "family": IntentFamily.WEB_SEARCH,
                "subtype": None,
                "entities": {"query": query},
                "analysis_data": {},
                "pipeline": self._select_pipeline(IntentFamily.WEB_SEARCH, "web_search"),
                "clean_text": query
            }

        # 1. Analyze Intent
        analysis_result = self.dispatcher.dispatch("analyze_intent", arguments={"text": text})

        intent = "unknown"
        entities = {}
        analysis_data = {}
        family = None
        subtype = None

        if analysis_result["status"] == "success":
            analysis_data = analysis_result["data"]
            if isinstance(analysis_data, dict):
                intent = analysis_data.get("intent", "unknown")
                entities = analysis_data.get("entities", {})
                # Respect analyzer's family classification if provided
                family = analysis_data.get("family")
                subtype = analysis_data.get("subtype")
        else:
            logger.warning(f"Intent analysis failed: {analysis_result['errors']}")

        # 2. Determine Family (if not provided by analyzer)
        if not family or family == "unknown":
            family = self.intent_family_map.get(intent, IntentFamily.QA) # Default to QA

        # 3. Select Pipeline
        pipeline = self._select_pipeline(family, intent)

        return {
            "intent": intent,
            "family": family,
            "subtype": subtype,
            "entities": entities,
            "analysis_data": analysis_data,
            "pipeline": pipeline
        }

    def execute_pipeline(self, pipeline: List[str], initial_input: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Executes the given pipeline of tools.
        """
        results = []
        context = context or {}
        previous_output = initial_input

        if not self._validate_pipeline(pipeline):
            return {
                "status": "error",
                "pipeline": pipeline,
                "error": "Invalid pipeline generated by router",
                "results": [],
                "final_output": None
            }

        # Update context with input
        context.update({"original_input": initial_input})
        entities = context.get("entities", {})

        for i, tool_name in enumerate(pipeline):
            step_args = {
                "text": previous_output,
                "entities": entities,
                "context": context
            }

            # Special handling: if tool is 'plan_steps', it might need 'goal'
            if tool_name == "plan_steps":
                step_args["goal"] = initial_input

            logger.info(f"Executing tool '{tool_name}' (step {i+1})")
            step_result = self.dispatcher.dispatch(tool_name, arguments=step_args)
            results.append(step_result)

            if step_result["status"] == "success":
                data = step_result["data"]
                if isinstance(data, str):
                    previous_output = data
                elif isinstance(data, dict) and "result" in data:
                    previous_output = data["result"]
                else:
                    previous_output = str(data)
            else:
                logger.error(f"Tool '{tool_name}' failed: {step_result['errors']}")
                break

        return {
            "pipeline": pipeline,
            "results": results,
            "final_output": previous_output
        }

    def route(self, text: str, pipeline_override: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Analyzes intent and executes the corresponding tool pipeline.
        """
        logger.info(f"Router received input: {text}")

        # Always analyze intent first to maintain context
        resolve_result = self.resolve(text)
        intent = resolve_result["intent"]
        family = resolve_result["family"]
        entities = resolve_result["entities"]
        analysis_data = resolve_result["analysis_data"]

        # Use clean text if provided (e.g. from web search override)
        execution_text = resolve_result.get("clean_text", text)

        if pipeline_override:
            pipeline = pipeline_override
            intent = "override"
            logger.info(f"Using overridden pipeline: {pipeline}")
        else:
            pipeline = resolve_result["pipeline"]
            if family == IntentFamily.WEB_SEARCH:
                logger.info(f"Routing to Web Search pipeline for query: '{text}'")
            logger.info(f"Routing intent: {intent} (Family: {family}), Selected pipeline: {pipeline}")

        context = {
            "intent": intent,
            "family": family,
            "entities": entities,
            "analysis": analysis_data
        }

        execution_result = self.execute_pipeline(pipeline, execution_text, context)

        return {
            "intent": intent,
            "family": family,
            "pipeline": pipeline,
            "results": execution_result.get("results", []),
            "final_output": execution_result.get("final_output"),
            "status": execution_result.get("status", "success"),
            "error": execution_result.get("error")
        }

    def _select_pipeline(self, family: str, intent: str) -> List[str]:
        pipeline = self.family_pipeline_map.get(family)
        if not pipeline:
            logger.info(f"No pipeline found for family '{family}'. Using fallback.")
            pipeline = self.fallback_pipeline
        return pipeline



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\scheduler.py
# ===========================================================================

import threading
import time
import logging
from datetime import datetime
from Autonomous_Reasoning_System.infrastructure.observability import Metrics
# from Autonomous_Reasoning_System.tools.action_executor import ActionExecutor # Removed dumb executor
from Autonomous_Reasoning_System.control.attention_manager import attention  # üß≠ added

logger = logging.getLogger(__name__)

lock = threading.Lock()  # global lock shared by the thread


def check_due_reminders(memory_storage, lookahead_minutes=1):
    """
    Scan stored memories for any 'task' entries due within ¬±lookahead_minutes,
    print reminders once, and mark them as 'triggered' to avoid repeats.
    """
    try:
        df = memory_storage.get_all_memories()
        if df.empty or "scheduled_for" not in df.columns:
            return

        now = datetime.utcnow()

        # Select untriggered reminders due now or very soon
        due = df[
            (df["memory_type"] == "task")
            & (df["scheduled_for"].notna())
            & (df["status"].isna() | (df["status"] != "triggered"))
            & ((df["scheduled_for"] - now).dt.total_seconds().abs() < lookahead_minutes * 60)
        ]

        if due.empty:
            return

        for _, row in due.iterrows():
            logger.info(f"‚è∞ Reminder: {row['text']} (scheduled {row['scheduled_for']})")

            # Mark reminder as triggered so it fires only once
            try:
                with memory_storage._write_lock:
                    memory_storage.con.execute(
                        "UPDATE memory SET status = 'triggered' WHERE id = ?",
                        (row["id"],)
                    )
                logger.info(f"‚úÖ Marked reminder '{row['text'][:40]}...' as triggered.")
            except Exception as e:
                logger.warning(f"[‚ö†Ô∏è ReminderUpdate] Failed to mark triggered: {e}")

    except Exception as e:
        logger.error(f"[‚ö†Ô∏è ReminderCheck] {e}")


def start_heartbeat_with_plans(learner, confidence, plan_builder, interval_seconds=90, test_mode=True, plan_executor=None):
    """
    Heartbeat loop with plan awareness.
    Periodically summarises learning, reminds Tyrone of active plans,
    checks due reminders, and autonomously executes the next pending step for each plan.
    """

    # Use passed plan_executor or warn
    if not plan_executor:
         logger.warning("[WARN] Scheduler running without robust PlanExecutor. Plan steps may fail.")

    def loop():
        time.sleep(3)  # let systems initialise first
        counter = 0
        while True:
            try:
                start_tick = time.time()
                # üß≠ Attention Check ‚Äî skip background work if user is active or recently interacted
                if attention.should_pause_autonomous():
                    # optional: only print occasionally to avoid clutter
                    # logger.info("[üß≠ ATTENTION] User or recent activity detected ‚Äî pausing background tasks.")
                    time.sleep(5)
                    continue

                with lock:  # prevent overlap
                    Metrics().increment("scheduler_heartbeat")
                    # --- learning summary ---
                    summary = learner.summarise_recent(window_minutes=2)
                    ts = datetime.now().strftime("%H:%M:%S")
                    logger.info(f"[üïí HEARTBEAT] {ts} ‚Üí {summary['summary']}")
                    if hasattr(confidence, "decay_all"):
                        confidence.decay_all()

                    # --- reminder check ---
                    check_due_reminders(learner.memory_storage if hasattr(learner, "memory_storage") else learner.memory)

                    # --- every few pulses, check active plans ---
                    counter += 1
                    if counter % 3 == 0:  # e.g. every 3 heartbeats
                        active = plan_builder.get_active_plans()
                        if active:
                            logger.info(f"[üìã ACTIVE PLANS] {len(active)} ongoing:")
                            for plan in active:
                                prog = plan.progress_summary()
                                logger.info(f"   ‚Ä¢ {plan.title}: {prog['completed_steps']}/{prog['total_steps']} steps complete.")

                                # üß† store reflection reminder
                                plan_builder.memory.add_memory(
                                    text=f"Reminder: Continue plan '{plan.title}'. Current step: {prog['current_step']}.",
                                    memory_type="plan_reminder",
                                    importance=0.3,
                                    source="Scheduler"
                                )

                                # ü§ñ attempt next step automatically
                                next_step = plan.next_step()
                                if next_step and next_step.status == "pending":
                                    logger.info(f"[ü§ñ EXECUTOR] Running next step for '{plan.title}': {next_step.description}")

                                    result_status = "failed"
                                    result_output = "No executor available"

                                    if plan_executor:
                                         # Use PlanExecutor's new execute_next_step method
                                         exec_res = plan_executor.execute_next_step(plan.id)

                                         status = exec_res.get("status")
                                         if status == "complete":
                                              result_status = "complete"
                                              result_output = "Plan finished!"
                                         elif status == "running":
                                              result_status = "running"
                                              result_output = f"Step completed: {exec_res.get('step_completed')}"
                                         elif status == "suspended":
                                              result_status = "suspended"
                                              result_output = f"Suspended: {exec_res.get('errors')}"
                                         else:
                                              result_status = "failed"
                                              result_output = str(exec_res.get("errors"))
                                    else:
                                         # Fallback/Dummy
                                         result_status = "failed"
                                         result_output = "PlanExecutor missing"

                                    # Plan updates are handled inside plan_executor usually.
                                    logger.info(f"[ü§ñ EXECUTOR] Result: {result_status}")

                        else:
                            logger.info("[üìã ACTIVE PLANS] None currently active.")

                # Record timing
                Metrics().record_time("scheduler_tick_duration", time.time() - start_tick)

            except Exception as e:
                logger.error(f"[‚ö†Ô∏è HEARTBEAT ERROR] {e}")
                Metrics().increment("scheduler_errors")

            time.sleep(interval_seconds)

    t = threading.Thread(target=loop, daemon=True)
    t.start()
    mode = "TEST" if test_mode else "NORMAL"
    logger.info(f"[‚è∞ HEARTBEAT+PLANS] Started ({mode} mode, interval={interval_seconds}s).")
    return t



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\control\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\api.py
# ===========================================================================

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
import uuid
import asyncio
from typing import AsyncGenerator

from Autonomous_Reasoning_System.control.core_loop import CoreLoop
from Autonomous_Reasoning_System.infrastructure.observability import HealthServer, Metrics

app = FastAPI(title="Tyrone Agent API", version="1.0.0")
tyrone = CoreLoop()
health_server = HealthServer(port=8001)
health_server.start()


class TaskRequest(BaseModel):
    goal: str


# ------------------------------------------------------------------
# 1. Submit a new goal ‚Üí starts in background
# ------------------------------------------------------------------
@app.post("/v1/task")
async def create_task(request: TaskRequest):
    plan_id = f"plan_{uuid.uuid4().hex[:12]}"
    tyrone.run_background(request.goal, plan_id)
    Metrics().increment("task_submitted")
    return {"plan_id": plan_id, "status": "queued"}


# ------------------------------------------------------------------
# 2. Poll status of a running / finished plan
# ------------------------------------------------------------------
@app.get("/v1/task/{plan_id}")
async def get_task_status(plan_id: str):
    status = tyrone.get_plan_status(plan_id)
    if not status:
        raise HTTPException(404, "Plan not found")
    return status


# ------------------------------------------------------------------
# 3. Real-time streaming of thoughts (SSE)
# ------------------------------------------------------------------
async def event_stream(plan_id: str) -> AsyncGenerator[str, None]:
    queue: asyncio.Queue = asyncio.Queue()
    tyrone.subscribe_stream(plan_id, queue)

    try:
        while True:
            line = await queue.get()
            if line is None:  # signals end
                break
            yield f"data: {line}\n\n"
            await asyncio.sleep(0.01)
    finally:
        tyrone.unsubscribe_stream(plan_id)


@app.get("/v1/stream/{plan_id}")
async def stream_task(plan_id: str):
    return StreamingResponse(event_stream(plan_id), media_type="text/event-stream")


# ------------------------------------------------------------------
# 4. Health
# ------------------------------------------------------------------
@app.get("/healthz")
async def healthz():
    return {"status": "healthy"}


if __name__ == "__main__":
    uvicorn.run("Autonomous_Reasoning_System.infrastructure.api:app", host="0.0.0.0", port=8000, reload=False)



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\concurrency.py
# ===========================================================================

import threading

# Global lock for memory write operations to ensure concurrency safety across modules
memory_write_lock = threading.Lock()



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\config.py
# ===========================================================================

"""
Global configuration for MyAssistant.
Loads values from environment variables or uses defaults.
"""
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Base data directory (relative to repo unless overridden)
DEFAULT_DATA_DIR = Path(__file__).parent.parent.parent / "data"
DATA_DIR = Path(os.getenv("DATA_DIR", DEFAULT_DATA_DIR))

# LLM provider placeholder (e.g., "ollama", "lmstudio", "openai", "groq", etc.)
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama")

# Default model name for local inference
DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "gemma3:1b")

# Ollama Base URL
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# Memory storage
MEMORY_DB_PATH = os.getenv("MEMORY_DB_PATH", str(DATA_DIR / "memory.duckdb"))
MEMORY_PARQUET_PATH = os.getenv("MEMORY_PARQUET_PATH", str(DATA_DIR / "memory.parquet"))

# WhatsApp Configuration
WA_USER_DATA_DIR = os.getenv("WA_USER_DATA_DIR", None)
WA_SELF_CHAT_URL = os.getenv("WA_SELF_CHAT_URL", "https://web.whatsapp.com")
WA_SELF_NAME = os.getenv("WA_SELF_NAME", "User")
WA_POLL_INTERVAL = int(os.getenv("WA_POLL_INTERVAL", "2"))



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\logging_utils.py
# ===========================================================================

import logging
import sys
import os
import json
from Autonomous_Reasoning_System.infrastructure import config

class StructuredFormatter(logging.Formatter):
    def format(self, record):
        # Basic structured format
        log_record = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "line": record.lineno
        }
        # Add exception info if present
        if record.exc_info:
            log_record["exception"] = self.formatException(record.exc_info)

        return json.dumps(log_record)

def setup_logging(default_level=logging.INFO):
    """
    Configures the root logger with a consistent format and handlers.
    """
    # Check if we want JSON logging via env var, default to standard for readability unless specified
    json_logging = os.getenv("JSON_LOGGING", "false").lower() == "true"

    # Define the log format
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"

    # Create a formatter
    if json_logging:
        formatter = StructuredFormatter(datefmt=date_format)
    else:
        formatter = logging.Formatter(log_format, date_format)

    # Configure the root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(default_level)

    # Remove existing handlers to avoid duplicate logs if re-configured
    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    # Create a console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # Create file handler in same directory as DB/memory files
    log_dir = os.path.dirname(config.MEMORY_DB_PATH) or "."
    os.makedirs(log_dir, exist_ok=True)
    file_path = os.path.join(log_dir, "tyrone.log")
    file_handler = logging.FileHandler(file_path, encoding="utf-8")
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

    logging.info("Logging system initialized.")

def get_logger(name):
    """
    Returns a logger with the specified name.
    """
    return logging.getLogger(name)



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\observability.py
# ===========================================================================

import threading
import time
import json
import logging
from http.server import HTTPServer, BaseHTTPRequestHandler
from collections import defaultdict

logger = logging.getLogger(__name__)

class Metrics:
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if not cls._instance:
            with cls._lock:
                if not cls._instance:
                    cls._instance = super(Metrics, cls).__new__(cls)
                    cls._instance._init()
        return cls._instance

    def _init(self):
        self.counters = defaultdict(int)
        self.gauges = defaultdict(float)
        self.histograms = defaultdict(list)
        self.start_time = time.time()

    def increment(self, name: str, value: int = 1):
        with self._lock:
            self.counters[name] += value

    def set_gauge(self, name: str, value: float):
        with self._lock:
            self.gauges[name] = value

    def record_time(self, name: str, duration: float):
        with self._lock:
            # Keep last 100 timings
            self.histograms[name].append(duration)
            if len(self.histograms[name]) > 100:
                self.histograms[name].pop(0)

    def get_metrics(self):
        with self._lock:
            metrics = {
                "uptime": time.time() - self.start_time,
                "counters": dict(self.counters),
                "gauges": dict(self.gauges),
                "timings": {k: {"avg": sum(v)/len(v) if v else 0, "count": len(v)} for k, v in self.histograms.items()}
            }
            return metrics

class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == "/healthz":
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"OK")
        elif self.path == "/metrics":
            self.send_response(200)
            self.send_header("Content-type", "application/json")
            self.end_headers()
            metrics = Metrics().get_metrics()
            self.wfile.write(json.dumps(metrics, indent=2).encode("utf-8"))
        else:
            self.send_response(404)
            self.end_headers()

    def log_message(self, format, *args):
        # Suppress default HTTP logging to avoid clutter
        pass

class HealthServer(threading.Thread):
    def __init__(self, port=8000):
        super().__init__()
        self.port = port
        self.daemon = True # Auto-kill when main thread exits
        self.httpd = None

    def run(self):
        try:
            self.httpd = HTTPServer(('0.0.0.0', self.port), HealthHandler)
            logger.info(f"üè• Healthz server listening on port {self.port}")
            self.httpd.serve_forever()
        except Exception as e:
            logger.error(f"Failed to start health server: {e}")

    def stop(self):
        if self.httpd:
            self.httpd.shutdown()



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\startup_validator.py
# ===========================================================================

import os
import sys
import logging
import duckdb
import pandas as pd
import pickle
from pathlib import Path
from Autonomous_Reasoning_System.infrastructure import config

logger = logging.getLogger(__name__)

def validate_startup():
    """
    Performs a fail-safe boot check.
    If any critical data file is missing or corrupted, it HALTS execution.
    It does NOT auto-rebuild.
    """
    print("[Startup Validator] Verifying system integrity...")

    # Determine Data Directory
    # We assume MEMORY_DB_PATH is like "data/memory.duckdb"
    db_path = Path(config.MEMORY_DB_PATH)
    data_dir = db_path.parent

    if not data_dir.exists():
        print(f"CRITICAL ERROR: Data directory not found at {data_dir}")
        sys.exit(1)

    # 1. Check DuckDB
    if not db_path.exists():
        print(f"CRITICAL ERROR: DuckDB file missing at {db_path}")
        sys.exit(1)

    try:
        con = duckdb.connect(str(db_path), read_only=True)
        # Check for required tables
        tables = con.execute("SHOW TABLES").fetchall()
        table_names = [t[0] for t in tables]
        required_tables = ["memory", "goals"]
        for rt in required_tables:
            if rt not in table_names:
                print(f"CRITICAL ERROR: DuckDB missing table '{rt}'")
                sys.exit(1)
        con.close()
    except Exception as e:
        print(f"CRITICAL ERROR: DuckDB corrupted or unreadable: {e}")
        sys.exit(1)

    # 2. Check Parquet Files
    parquet_files = ["memory.parquet", "goals.parquet", "episodes.parquet"]
    for p_file in parquet_files:
        p_path = data_dir / p_file
        if not p_path.exists():
            print(f"CRITICAL ERROR: Parquet file missing: {p_path}")
            sys.exit(1)
        try:
            pd.read_parquet(p_path)
        except Exception as e:
            print(f"CRITICAL ERROR: Parquet file corrupted: {p_path} ({e})")
            sys.exit(1)

    print("[Startup Validator] System integrity verified. Proceeding to boot.")



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\infrastructure\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\io\pdf_ingestor.py
# ===========================================================================

import logging
from pathlib import Path
from pypdf import PdfReader
import textwrap
from Autonomous_Reasoning_System.memory.storage import MemoryStorage
from Autonomous_Reasoning_System.memory.llm_summarizer import summarize_with_local_llm
# We only import these for the default case
from Autonomous_Reasoning_System.memory.embeddings import EmbeddingModel
from Autonomous_Reasoning_System.memory.vector_store import DuckVSSVectorStore

logger = logging.getLogger(__name__)

class PDFIngestor:
    """
    Loads a PDF, extracts text, chunks it, and stores each part as a memory.
    Optionally creates an overall summary.
    """
    def __init__(self, memory_storage=None):
        # If an existing storage (Tyrone's brain) is passed, use it.
        if memory_storage:
            self.memory = memory_storage
            # We assume the storage already has an embedder/vector_store attached
            self.embedder = memory_storage.embedder
        else:
            # Fallback: Create a standalone stack (Legacy behavior, fixes the warning too)
            logger.info("PDFIngestor: Initializing standalone memory stack...")
            self.embedder = EmbeddingModel()
            self.vector_store = DuckVSSVectorStore()
            self.memory = MemoryStorage(
                embedding_model=self.embedder,
                vector_store=self.vector_store
            )

    def ingest(self, file_path: str, chunk_size: int = 1000, summarize: bool = True):
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        logger.info(f"üìÑ Reading PDF: {path.name}")
        try:
            reader = PdfReader(path)
            text = "\n".join(page.extract_text() or "" for page in reader.pages)
        except Exception as e:
            logger.error(f"Failed to read PDF: {e}")
            return

        if not text.strip():
            logger.warning("‚ö†Ô∏è No text extracted.")
            return

        # Split into chunks
        chunks = textwrap.wrap(text, chunk_size)
        logger.info(f"üß© Splitting into {len(chunks)} chunks...")

        for i, chunk in enumerate(chunks, 1):
            title = f"{path.stem} (Part {i}/{len(chunks)})"
            self.memory.add_memory(
                text=f"{title}\n\n{chunk}",
                memory_type="document",
                importance=0.7,
                source=path.name,
            )

        if summarize:
            logger.info("üß† Summarizing content...")
            summary = summarize_with_local_llm(text[:6000])  # limit for speed
            self.memory.add_memory(
                text=f"Summary of {path.name}:\n{summary}",
                memory_type="document_summary",
                importance=0.9,
                source="PDFIngestor"
            )
            logger.info("üßæ Summary added to memory.")

        logger.info(f"‚úÖ Ingestion complete: {len(chunks)} chunks + summary stored.")

if __name__ == "__main__":
    # Standalone test usage
    import sys
    from Autonomous_Reasoning_System.infrastructure.logging_utils import setup_logging
    setup_logging()

    if len(sys.argv) < 2:
        print("Usage: python -m Autonomous_Reasoning_System.io.pdf_ingestor <pdf_path>")
        sys.exit(1)

    ingestor = PDFIngestor() # Will use fallback init
    ingestor.ingest(sys.argv[1])



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\io\read_from_memory_test.py
# ===========================================================================

from Autonomous_Reasoning_System.memory.storage import MemoryStorage
import pandas as pd
import textwrap

def read_document_from_memory(source_name: str = None, limit: int = 20):
    memory = MemoryStorage()
    df = memory.get_all_memories()

    if not isinstance(df, pd.DataFrame):
        print("‚ö†Ô∏è Storage did not return a DataFrame.")
        return

    print(f"üß† Retrieved {len(df)} records with columns: {list(df.columns)}")

    # Try to filter by source name if provided
    if source_name:
        df = df[df["source"].str.contains(source_name, case=False, na=False)]

    if df.empty:
        print(f"No matching entries for '{source_name}', showing first {limit} records instead.\n")
        df = memory.get_all_memories().head(limit)

    for i, row in df.iterrows():
        print(f"\n--- Memory {i+1} / {len(df)} ---")
        text = row.get("text", "")
        print("\n".join(textwrap.wrap(text, width=100)))
        print()

if __name__ == "__main__":
    import sys
    source = sys.argv[1] if len(sys.argv) > 1 else None
    read_document_from_memory(source)



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\io\whatsapp.py
# ===========================================================================

import queue
import sys
import threading
import time
from collections import deque
from Autonomous_Reasoning_System.control.core_loop import CoreLoop
from Autonomous_Reasoning_System.infrastructure import config
from playwright.sync_api import sync_playwright


USER_DATA_DIR = config.WA_USER_DATA_DIR
SELF_CHAT_URL = config.WA_SELF_CHAT_URL
POLL_INTERVAL = config.WA_POLL_INTERVAL
SELF_NAME = config.WA_SELF_NAME

SELF_PREFIXES = ("noted", "task noted", "sorry", "‚ö†Ô∏è", "error", "ok", "done")
SENT_CACHE = deque(maxlen=10)

LAST_OUTGOING = None  # tracks last message sent

tyrone = CoreLoop()

def handle_message(text: str):
    out = tyrone.run_once(text)
    return out.get("summary", "(no summary)")


# --------------------------------------------------------------------------
# üëá Core utility methods (untouched)
# --------------------------------------------------------------------------

def is_from_self(text: str) -> bool:
    if not text:
        return False
    lowered = text.strip().lower()
    return any(lowered.startswith(p) for p in SELF_PREFIXES)


def find_input(page):
    strict_selector = 'div[contenteditable="true"][role="textbox"][aria-label^="Type"]'
    if page.query_selector(strict_selector):
        return strict_selector
    for sel in [
        'div[contenteditable="true"][data-tab="10"]',
        'div[aria-placeholder="Type a message"]',
        'footer div[contenteditable="true"]',
        'div._ak1l',
    ]:
        if page.query_selector(sel):
            return sel
    return None


def wait_for_input_box(page, timeout=60000):
    start = time.time()
    while time.time() - start < timeout / 1000:
        sel = find_input(page)
        if sel:
            return sel
        time.sleep(0.5)
    raise RuntimeError("Message input box not found.")


def is_just_sent(text: str) -> bool:
    if not text:
        return False
    return text.strip() in SENT_CACHE


def send_message(page, text):
    sel = find_input(page)
    if not sel:
        sel = wait_for_input_box(page, timeout=5000)
    if not sel:
        raise RuntimeError("Message input not found.")

    page.click(sel)

    lines = text.split("\n")
    for i, line in enumerate(lines):
        page.type(sel, line)
        if i != len(lines) - 1:
            page.keyboard.down("Shift")
            page.keyboard.press("Enter")
            page.keyboard.up("Shift")

    page.keyboard.press("Enter")

    SENT_CACHE.append(text.strip())
    global LAST_OUTGOING
    LAST_OUTGOING = text.strip()


def clean_quotes(text):
    text = text.strip()
    if (text.startswith('"') and text.endswith('"')) or (
        text.startswith("'") and text.endswith("'")
    ):
        return text[1:-1]
    return text


def command_reader(cmd_queue, stop_event):
    while not stop_event.is_set():
        try:
            cmd = input("> ")
        except (EOFError, KeyboardInterrupt):
            cmd_queue.put("exit")
            break
        cmd = cmd.strip()
        cmd_queue.put(cmd)
        if cmd.lower() == "exit":
            break


def read_last_message_text(page):
    js = """
    () => {
      const msgs = Array.from(document.querySelectorAll('div[role="row"] span.selectable-text'));
      if (!msgs.length) return null;
      return msgs[msgs.length - 1].innerText.trim();
    }
    """
    try:
        return page.evaluate(js)
    except Exception as e:
        print(f"[DEBUG] Error in read_last_message_text: {e}")
        return None


# --------------------------------------------------------------------------
# ‚úÖ NEW: refactored message processor (lightweight)
# --------------------------------------------------------------------------

def process_incoming_message(page, message_text):
    cleaned = message_text.strip()
    if cleaned in ("```", "''", '""', "`", "'''"):
        print(f"[DEBUG] Ignoring noise message: {cleaned}")
        return

    lowered = cleaned.lower()
    blocked_starts = (
        "tyrone>", "*", "-", "‚Ä¢",
        "i cannot fulfill",
        "here are the stored birthdays"
    )
    if any(lowered.startswith(b) for b in blocked_starts):
        print(f"[DEBUG] Skipping blocked/self message: {message_text}")
        return

    if is_just_sent(cleaned):
        print(f"[DEBUG] Skipping echo of sent message: {cleaned}")
        return

    wh_starts = ("when ", "what ", "where ", "who ", "how ", "why ")
    if any(lowered.startswith(w) for w in wh_starts) and not cleaned.endswith("?"):
        cleaned = cleaned + "?"
        print(f"[DEBUG] Auto-appended '?': {cleaned}")

    print(f"[DEBUG] Processing incoming: {cleaned}")

    try:
        # üîÅ now routed through workspace
        reply = handle_message(cleaned)
        if reply:
            formatted = f"Tyrone> {reply}"
            print(f"[DEBUG] Sending reply: {formatted}")
            send_message(page, formatted)
    except Exception as e:
        print(f"Error while processing message: {e}")
        try:
            send_message(page, "‚ö†Ô∏è Error handling your message.")
        except:
            pass


# --------------------------------------------------------------------------
# üöÄ Main runner loop
# --------------------------------------------------------------------------

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=False,
        )
        page = browser.new_page()
        page.goto(SELF_CHAT_URL)

        print("‚è≥ Loading WhatsApp...")

        try:
            wait_for_input_box(page)
            print("‚úÖ WhatsApp ready and self-chat loaded.")
            print(">> You can now type 'send <message>' or 'exit' below. <<")
            last_seen_text = read_last_message_text(page)
            startup_boundary = last_seen_text
            print(f"[DEBUG] Startup boundary is: {startup_boundary}")
        except Exception as e:
            print("‚ùå Could not load WhatsApp:", e)
            browser.close()
            sys.exit(1)

        print("Listening for messages and commands...")
        print("Commands: send <message> | exit")

        cmd_queue = queue.Queue()
        stop_event = threading.Event()
        input_thread = threading.Thread(
            target=command_reader,
            args=(cmd_queue, stop_event),
            daemon=True,
        )
        input_thread.start()

        ready_for_messages = True

        try:
           while True:
                current_message = read_last_message_text(page)

                if current_message and current_message != last_seen_text:
                    print(f"[DEBUG] New message detected: {current_message}")

                    # ‚úÖ Ignore if it's part of the last outgoing message (multi-line echo protection)
                    if LAST_OUTGOING and current_message.strip() in LAST_OUTGOING:
                        print(f"[DEBUG] Ignoring echo (substring of last outgoing): {current_message}")
                        last_seen_text = current_message
                        continue

                    if not is_just_sent(current_message) and not is_from_self(current_message):
                        print(f"\nüì© INCOMING: {current_message}")
                        process_incoming_message(page, current_message)

                    last_seen_text = current_message


                # Commands
                try:
                    cmd = cmd_queue.get(timeout=POLL_INTERVAL)
                except queue.Empty:
                    continue

                if not cmd:
                    continue
                if cmd.lower() == "exit":
                    break
                if cmd.lower().startswith("send "):
                    text = clean_quotes(cmd[5:].strip())
                    if text:
                        try:
                            send_message(page, text)
                            print(f"‚úÖ SENT: {text}")
                        except Exception as e:
                            print(f"‚ùå Failed to send: {e}")
                    else:
                        print("‚ÑπÔ∏è No message to send.")
                else:
                    print("Unrecognized command. Commands: send <message> | exit")

        finally:
            stop_event.set()
            if input_thread.is_alive():
                input_thread.join(timeout=1)
            try:
                browser.close()
            except Exception:
                print("‚ö†Ô∏è Browser was already closed or disconnected.")
            print("\n‚úÖ Closed cleanly.")


if __name__ == "__main__":
    main()



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\io\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\consolidator.py
# ===========================================================================

# Autonomous_Reasoning_System/llm/consolidator.py

from .engine import call_llm


class ReasoningConsolidator:
    """
    Periodically summarizes recent episodes into long-term summaries.
    """

    def __init__(self, memory_storage=None):
        self.memory = memory_storage

    def consolidate_recent(self, limit: int = 5):
        """
        Fetches recent episodic memories and generates a concise summary.
        """
        if not self.memory:
            return "Memory storage not available."

        try:
            df = self.memory.search_memory("Assistant:")
            if df.empty:
                return "No episodic memories to summarize."

            # Take most recent episodes
            subset = df.sort_values("created_at", ascending=False).head(limit)
            text_block = "\n\n".join(subset["text"].tolist())

            # Summarize via LLM
            prompt = (
                "Summarize the following conversation snippets into one short paragraph "
                "describing what the assistant has recently been focused on.\n\n"
                f"{text_block}"
            )
            summary = call_llm(
                "You are an episodic summarizer that writes short, coherent summaries.",
                prompt
            )

            # Store the summary as a long-term episodic memory
            self.memory.add_memory(
                text=f"Session Summary: {summary}",
                memory_type="episodic_summary",
                importance=0.9,
                source="consolidator"
            )

            return summary

        except Exception as e:
            return f"[ReasoningConsolidator Error] {e}"



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\context_adapter.py
# ===========================================================================

from ..memory.context_builder import ContextBuilder
from ..memory.retrieval_orchestrator import RetrievalOrchestrator
from .engine import call_llm
from .consolidator import ReasoningConsolidator
from ..tools.system_tools import get_current_time, get_current_location
import threading
import logging

logger = logging.getLogger(__name__)

class ContextAdapter:
    """
    Connects Tyrone's memory context to the reasoning engine.
    Embedding-based, entity-agnostic context integration.
    Retrieves the top semantically and deterministically relevant
    memories and ensures the LLM treats them as verified truth.
    """

    CONSOLIDATION_INTERVAL = 5  # summarize every N turns

    def __init__(self, memory_storage=None, embedding_model=None):
        self.builder = ContextBuilder()
        self.memory = memory_storage
        if not self.memory:
             logger.warning("[WARN] ContextAdapter initialized without memory_storage.")

        self.retriever = RetrievalOrchestrator(memory_storage=self.memory, embedding_model=embedding_model)
        self.consolidator = ReasoningConsolidator()
        self.turn_counter = 0
        self.history = [] # Short-term conversation history
        self.startup_context = {}  # Stores startup info like time and location
        self._context_lock = threading.Lock()

        # Load recent conversation history from persistence
        if self.memory:
            try:
                # Handle both MemoryStorage and MemoryInterface wrapper
                storage_ref = self.memory
                if hasattr(self.memory, "storage"):
                    storage_ref = self.memory.storage

                if hasattr(storage_ref, "get_all_memories"):
                    df = storage_ref.get_all_memories()
                    if not df.empty:
                        # Filter for episodes
                        episodes = df[df["memory_type"] == "episode"]
                        if not episodes.empty:
                            # Sort by created_at
                            episodes = episodes.sort_values("created_at", ascending=True)

                            # Load last 10 interactions (approx 20 lines)
                            recent = episodes.tail(10)

                            for _, row in recent.iterrows():
                                text = str(row.get("text", ""))
                                # Split into lines to match history structure (User line, Tyrone line)
                                for line in text.split("\n"):
                                    if line.strip():
                                        self.history.append(line.strip())

                            # Keep only the most recent 20 lines to avoid token bloat on restart
                            self.history = self.history[-20:]

                            logger.info(f"[ContextAdapter] Restored {len(self.history)} lines of conversation history.")
            except Exception as e:
                logger.error(f"[ContextAdapter] Error loading history: {e}")

    def set_startup_context(self, context: dict):
        """Sets the startup context (e.g. location, time)."""
        with self._context_lock:
            self.startup_context = context

    def _ensure_context(self):
        """Ensures minimal context exists if startup_context is empty."""
        # Double check locking pattern to avoid overhead if already set
        if self.startup_context:
            return

        with self._context_lock:
            if not self.startup_context:
                try:
                    # Fallback if not initialized externally
                    self.startup_context = {
                        "Current Time": get_current_time(),
                        # Skipping location to avoid API latency if not strictly needed, or we can call it.
                        # "Current Location": get_current_location()
                    }
                except Exception as e:
                    logger.warning(f"Failed to lazy-load context: {e}")

    # ------------------------------------------------------------------
    def run(self, user_input: str, system_prompt: str = None) -> str:
        self.turn_counter += 1

        self._ensure_context()

        memories = self.retriever.retrieve(user_input)

        memory_text = ""
        if memories:
            clean = [str(m).strip() for m in memories if str(m).strip()]
            if clean:
                memory_text = "\n".join(f"- {line}" for line in clean)

        # Build context window from history
        history_text = ""
        if self.history:
             history_text = "\nRECENT CONVERSATION:\n" + "\n".join(self.history[-5:]) + "\n"

        # Build startup context string
        startup_info = ""
        with self._context_lock:
            if self.startup_context:
                startup_info = "\nCURRENT CONTEXT:\n"
                for key, value in self.startup_context.items():
                    startup_info += f"- {key}: {value}\n"

        if memory_text or history_text or startup_info:
            system_prompt = f"""
YOU ARE TYRONE.

{startup_info}
LONG TERM MEMORY (FACTS):
{memory_text}

{history_text}

RULES YOU MUST OBEY:
- The LONG TERM MEMORY (FACTS) are verified truth and override ALL other knowledge, including the current system date in CURRENT CONTEXT, for any personal information.
- Use the facts above and the recent conversation context to answer.
- If the user asks about Cornelia's birthday, answer with the exact date from the facts.
- Never say "I haven't been told" when the fact is right here.
- Answer directly and naturally.

User question: {user_input}
Answer:
"""
            user_prompt = ""
        else:
            system_prompt = "You are Tyrone. No relevant memories found."
            user_prompt = user_input

        reply = call_llm(system_prompt=system_prompt, user_prompt=user_prompt)

        # Update History
        self.history.append(f"User: {user_input}")
        self.history.append(f"Tyrone: {reply}")

        if self.memory:
            self.memory.add_memory(
                text=f"User: {user_input}\nTyrone: {reply}",
                memory_type="episode",
                importance=0.7
            )

        return reply



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\embeddings.py
# ===========================================================================

# memory/embeddings.py
import numpy as np
from sentence_transformers import SentenceTransformer

class EmbeddingModel:
    """
    Thin wrapper around SentenceTransformer for vector generation.
    """
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)

    def embed(self, text: str) -> np.ndarray:
        if not text or not text.strip():
            return np.zeros(384, dtype=np.float32)
        vec = self.model.encode([text], normalize_embeddings=True)
        return vec[0]



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\engine.py
# ===========================================================================

import requests
import json
import time
import logging
from ..infrastructure import config
from ..infrastructure.observability import Metrics
from datetime import datetime
from zoneinfo import ZoneInfo


DEFAULT_MODEL = config.DEFAULT_MODEL or "gemma3:1b"
BASE_URL = config.OLLAMA_BASE_URL or "http://localhost:11434"

logger = logging.getLogger(__name__)

# Circuit Breaker State
_CIRCUIT_OPEN = False
_CIRCUIT_OPEN_UNTIL = 0
_CONSECUTIVE_FAILURES = 0
_FAILURE_THRESHOLD = 3
_RESET_TIMEOUT = 30

class LLMEngine:
    def __init__(self, provider: str = None, model: str = None):
        self.provider = provider or config.LLM_PROVIDER
        self.model = model or DEFAULT_MODEL

    def generate_response(self, prompt: str) -> str:
        """
        Dummy fallback. Replace later.
        """
        return call_llm("You are a helpful assistant.", prompt)

    def classify_memory(self, text: str):
        lower = text.lower()
        if "remind" in lower or "meeting" in lower or "schedule" in lower:
            return {"type": "task", "importance": 0.5}
        if "squash" in lower or "appointment" in lower or "call" in lower:
            return {"type": "event", "importance": 0.5}
        return {"type": "misc", "importance": 0.5}

    def embed_text(self, text: str):
        return None


# ‚úÖ MODULE-LEVEL function (not inside class!)
def call_llm(system_prompt: str, user_prompt: str, retries: int = 2) -> str:
    """
    Wraps the LLM call using HTTP requests to Ollama.
    Includes retry logic, timeouts, and circuit breaker for reliability.
    """
    global _CIRCUIT_OPEN, _CIRCUIT_OPEN_UNTIL, _CONSECUTIVE_FAILURES

    # Check Circuit Breaker
    if _CIRCUIT_OPEN:
        if time.time() < _CIRCUIT_OPEN_UNTIL:
            logger.warning("[LLM] Circuit breaker open. Skipping call.")
            return "Error: AI service temporarily suspended due to repeated failures."
        else:
            logger.info("[LLM] Circuit breaker reset timeout expired. Retrying...")
            _CIRCUIT_OPEN = False
            _CONSECUTIVE_FAILURES = 0

    merged_system = system_prompt
    full_prompt = f"SYSTEM:\n{merged_system}\n\nUSER:\n{user_prompt}"

    url = f"{BASE_URL}/api/generate"
    payload = {
        "model": DEFAULT_MODEL,
        "prompt": full_prompt,
        "stream": False
    }

    attempt = 0
    backoff = 1
    # Retries is number of retries, so total attempts = retries + 1
    total_attempts = retries + 1

    while attempt < total_attempts:
        start_time = time.time()
        try:
            attempt += 1
            # Timeout reduced to 15s as per requirement
            response = requests.post(url, json=payload, timeout=15)
            response.raise_for_status()

            latency = time.time() - start_time
            Metrics().record_time("llm_latency", latency)

            try:
                data = response.json()
            except json.JSONDecodeError:
                logger.warning(f"[LLM] Failed to decode JSON response (attempt {attempt}).")
                raise

            raw_out = data.get("response", "").strip()

            if not raw_out:
                logger.warning(f"[LLM] Empty response from model (attempt {attempt}).")
                if attempt < total_attempts:
                    time.sleep(backoff)
                    backoff *= 2
                    continue
                # Treat empty response as failure for circuit breaker?
                # Usually empty response is better than crash, but if persistent...
                # Let's treat it as success but empty content to avoid breaking circuit on logic issues.
                _CONSECUTIVE_FAILURES = 0
                return f"(no response content)"

            # Success
            _CONSECUTIVE_FAILURES = 0
            return raw_out

        except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):
            logger.warning(f"[LLM] Timeout waiting for response (attempt {attempt}).")
        except requests.exceptions.ConnectionError:
            logger.warning(f"[LLM] Could not connect to Ollama at {url}. Is it running? (attempt {attempt})")
        except Exception as e:
            logger.error(f"[LLM] Unexpected error: {e}")

        if attempt < total_attempts:
            time.sleep(backoff)
            backoff *= 2

    # If we reach here, all attempts failed
    _CONSECUTIVE_FAILURES += 1
    logger.error(f"[LLM] Call failed after {total_attempts} attempts. Consecutive failures: {_CONSECUTIVE_FAILURES}")

    if _CONSECUTIVE_FAILURES >= _FAILURE_THRESHOLD:
        _CIRCUIT_OPEN = True
        _CIRCUIT_OPEN_UNTIL = time.time() + _RESET_TIMEOUT
        logger.critical(f"[LLM] Circuit breaker ACTIVATED. Pausing LLM calls for {_RESET_TIMEOUT}s.")

    return "Error: AI service unavailable after retries."



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\plan_reasoner.py
# ===========================================================================

from Autonomous_Reasoning_System.llm.reflection_interpreter import ReflectionInterpreter
import re


class PlanReasoner(ReflectionInterpreter):
    def __init__(self, memory_storage=None, embedding_model=None):
        super().__init__(memory_storage=memory_storage, embedding_model=embedding_model)

    def generate_steps(self, goal_text: str) -> list[str]:
        prompt = f"Decompose this goal into 3-7 concrete, actionable steps:\n\n{goal_text}"
        raw = self.interpret(prompt)
        steps = re.split(r'\n\d+\.|\n-|\n‚Ä¢', str(raw))
        steps = [s.strip() for s in steps if s.strip() and len(s.strip()) > 10]
        return steps[:10] or ["Review and clarify the goal."]



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\reflection_interpreter.py
# ===========================================================================

import json
import re
from ..memory.retrieval_orchestrator import RetrievalOrchestrator
from .engine import call_llm


class ReflectionInterpreter:
    """
    Handles introspective or reflective reasoning.
    Now integrates factual context from Tyrone's memory before reflection.
    The model is instructed that retrieved memories override all other world knowledge.
    """

    def __init__(self, memory_storage=None, embedding_model=None):
        self.memory = memory_storage
        self.retriever = RetrievalOrchestrator(memory_storage=memory_storage, embedding_model=embedding_model)

    # ----------------------------------------------------------------------
    def interpret(self, user_input: str, raw: bool = False) -> dict:
        """
        Reflect on user input using Tyrone's stored experiences + factual memories.
        If raw=True, return unprocessed model output directly.
        Otherwise, return structured JSON with summary, insight, and confidence_change.
        """
        # === RAW MODE ===========================================================
        if raw:
            try:
                raw_response = call_llm("", user_input)
                return raw_response if isinstance(raw_response, str) else str(raw_response)
            except Exception as e:
                print(f"[WARN] ReflectionInterpreter raw mode failed: {e}")
                return ""

        # === REFLECTIVE MODE ====================================================
        try:
            df = self.memory.get_all_memories() if self.memory else None

            if df is None or df.empty:
                return {
                    "summary": "No reflections recorded yet.",
                    "insight": "Tyrone has not reflected before.",
                    "confidence_change": "neutral",
                }

            # üß† Retrieve relevant factual memories using semantic recall
            # Access protected method if available, or public retrieve
            if hasattr(self.retriever, "_semantic_retrieve"):
                retrieved = self.retriever._semantic_retrieve(user_input, k=5)
            else:
                retrieved = self.retriever.retrieve(user_input) # Fallback

            memory_context = (
                "\n".join([f"- {r}" for r in retrieved])
                if retrieved
                else "(no relevant factual memories found)"
            )

            # üß© Collect recent reflections + episodic summaries
            reflections = df[df["memory_type"].isin(["reflection", "episodic_summary"])]
            reflections = reflections.sort_values("created_at", ascending=False).head(8)
            reflection_block = "\n\n".join(reflections["text"].tolist())

            # üß≠ System prompt enforcing factual override
            system_prompt = (
                "You are Tyrone‚Äôs reflection module. "
                "The text below contains verified factual memories followed by self-reflection logs. "
                "Facts override all other world knowledge. "
                "Never introduce fictional or unrelated information. "
                "If a factual memory mentions a person, event, or detail, treat it as true. "
                "Analyze the reflections in light of the user's input and these facts. "
                "Respond only in valid JSON of the form:\n"
                '{"summary": "<short summary>", "insight": "<lesson or fact>", "confidence_change": "<positive|neutral|negative>"}'
            )

            user_prompt = (
                f"[FACTUAL CONTEXT]\n{memory_context}\n\n"
                f"[SELF REFLECTIONS]\n{reflection_block}\n\n"
                f"[USER INPUT]\n{user_input}\n\n"
                "Return only the JSON object."
            )

            print("\n[üß† FACTUAL CONTEXT FOR REFLECTION]")
            print(memory_context[:500])
            raw_output = call_llm(system_prompt=system_prompt, user_prompt=user_prompt)


            # --- Robust JSON extraction ---
            try:
                match = re.search(r"\{.*\}", raw_output, re.DOTALL)
                result = json.loads(match.group()) if match else json.loads(raw_output)
            except Exception:
                return {
                    "summary": "Raw reflection",
                    "insight": raw_output.strip(),
                    "confidence_change": "neutral",
                }

            # Fill defaults safely
            result.setdefault("summary", "(no summary)")
            result.setdefault("insight", "(no insight)")
            result.setdefault("confidence_change", "neutral")

            # üß© Log structured reflection
            if self.memory:
                self.memory.add_memory(
                    f"Reflection ‚Üí {result['summary']} | Insight: {result['insight']} | Confidence: {result['confidence_change']}",
                    memory_type="reflection",
                )

            print(f"[ü™û REFLECTION] {result}")
            return result

        except Exception as e:
            return {
                "summary": "ReflectionInterpreter error.",
                "insight": str(e),
                "confidence_change": "neutral",
            }



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\llm\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\memory\storage.py
# ===========================================================================

import json
import logging
import threading
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Any, Dict
from uuid import uuid4

import duckdb
from fastembed import TextEmbedding

from Autonomous_Reasoning_System import config

logger = logging.getLogger("ARS_Memory")

class MemoryStorage:
    """
    The Vault (FastEmbed Edition + Config + Batching).
    """

    def __init__(self, db_path: str = config.MEMORY_DB_PATH):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        logger.info(f"Loading FastEmbed from config: {config.EMBEDDING_MODEL_NAME}...")
        start_t = time.time()
        self.embedder = TextEmbedding(model_name=config.EMBEDDING_MODEL_NAME)
        self.vector_dim = config.VECTOR_DIMENSION
        logger.info(f"Model loaded ({time.time() - start_t:.2f}s)")

        self._lock = threading.RLock()
        logger.info(f"Connecting to DuckDB at {self.db_path}...")
        self.con = duckdb.connect(str(self.db_path))

        self._init_schema()
        logger.info("Database Ready.")

    def _get_embedding(self, text: str) -> list:
        # FastEmbed returns a generator of embeddings ‚Äî take first, convert to list
        embedding = next(self.embedder.embed([text]))
        return embedding.tolist() if hasattr(embedding, 'tolist') else list(embedding)

    def _init_schema(self) -> None:
        with self._lock:
            try:
                self.con.execute("INSTALL vss; LOAD vss;")
                self.con.execute("SET hnsw_enable_experimental_persistence = true;")
            except Exception:
                pass

            self.con.execute(
                "CREATE TABLE IF NOT EXISTS memory "
                "(id VARCHAR PRIMARY KEY, text VARCHAR, memory_type VARCHAR, "
                "created_at TIMESTAMP, importance DOUBLE, source VARCHAR, metadata JSON)"
            )
            self.con.execute(
                f"CREATE TABLE IF NOT EXISTS vectors "
                f"(id VARCHAR PRIMARY KEY, embedding FLOAT[{self.vector_dim}], "
                "FOREIGN KEY (id) REFERENCES memory(id))"
            )
            try:
                self.con.execute(
                    "CREATE INDEX IF NOT EXISTS idx_vec ON vectors USING HNSW (embedding) WITH (metric = 'cosine');"
                )
            except Exception:
                pass

            self.con.execute(
                "CREATE TABLE IF NOT EXISTS triples "
                "(subject VARCHAR, relation VARCHAR, object VARCHAR, PRIMARY KEY(subject, relation, object))"
            )
            self.con.execute(
                "CREATE TABLE IF NOT EXISTS plans "
                "(id VARCHAR PRIMARY KEY, goal_text VARCHAR, steps JSON, "
                "status VARCHAR, created_at TIMESTAMP, updated_at TIMESTAMP)"
            )

    def remember_batch(self,
                       texts: List[str],
                       memory_type: str = "episodic",
                       importance: float = 0.5,
                       source: str = "user",
                       metadata_list: Optional[List[dict]] = None) -> None:
        if not texts:
            return

        logger.debug(f"Batch processing {len(texts)} items...")
        t_start = time.time()
        embeddings = list(self.embedder.embed(texts))

        now = datetime.utcnow()
        with self._lock:
            self.con.execute("BEGIN TRANSACTION")
            try:
                for i, text in enumerate(texts):
                    mem_id = str(uuid4())
                    vector = embeddings[i].tolist()

                    meta = metadata_list[i] if metadata_list and i < len(metadata_list) else {}
                    meta['chunk_index'] = i
                    meta_json = json.dumps(meta)

                    self.con.execute(
                        "INSERT INTO memory VALUES (?, ?, ?, ?, ?, ?, ?)",
                        (mem_id, text, memory_type, now, importance, source, meta_json)
                    )
                    self.con.execute("INSERT INTO vectors VALUES (?, ?)", (mem_id, vector))

                    if meta.get('kg_triples'):
                        for s, r, o in meta['kg_triples']:
                            self.add_triple(s, r, o)
                self.con.execute("COMMIT")
                logger.debug(f"Batch saved in {time.time() - t_start:.2f}s")
            except Exception as e:
                self.con.execute("ROLLBACK")
                logger.error(f"Batch failed: {e}")
                raise e

    def get_whole_document(self, filename: str) -> str:
        """
        Retrieves all chunks associated with a specific filename,
        ordered correctly, and joins them into a single string.
        """
        logger.debug(f"Reassembling document: {filename}...")
        with self._lock:
            # We sort by created_at (for different upload sessions)
            # AND metadata->chunk_index (for order within a session)
            query = """
                SELECT text
                FROM memory
                WHERE source = ?
                ORDER BY created_at ASC, CAST(json_extract(metadata, '$.chunk_index') AS INTEGER) ASC
            """
            results = self.con.execute(query, (filename,)).fetchall()

        if not results:
            return f"No document found with name: {filename}"

        # Join chunks with a newline or space
        full_text = "\n".join([r[0] for r in results])
        return full_text

    def remember(self,
                 text: str,
                 memory_type: str = "episodic",
                 importance: float = 0.5,
                 source: str = "user",
                 metadata: Optional[dict] = None) -> None:
        return self.remember_batch([text], memory_type, importance, source, [metadata] if metadata else None)

    def add_triple(self, subj: str, rel: str, obj: str) -> None:
        self.con.execute(
            "INSERT OR IGNORE INTO triples VALUES (?, ?, ?)",
            (subj.lower(), rel.lower(), obj.lower())
        )

    def update_plan(self, plan_id: str, goal_text: str, steps: List[str], status: str = "active") -> None:
        now = datetime.utcnow()
        steps_json = json.dumps(steps)
        with self._lock:
            if self.con.execute("SELECT 1 FROM plans WHERE id=?", (plan_id,)).fetchone():
                self.con.execute(
                    "UPDATE plans SET steps=?, status=?, updated_at=? WHERE id=?",
                    (steps_json, status, now, plan_id)
                )
            else:
                self.con.execute(
                    "INSERT INTO plans VALUES (?, ?, ?, ?, ?, ?)",
                    (plan_id, goal_text, steps_json, status, now, now)
                )

    def search_similar(self, query: str, limit: int = 5, threshold: float = 0.4) -> List[Dict[str, Any]]:
        query_vec = self._get_embedding(query)
        with self._lock:
            results = self.con.execute(f"""
                SELECT substr(m.text, 1, 500), m.memory_type, m.created_at, (1 - list_cosine_similarity(v.embedding, ?::FLOAT[{self.vector_dim}])) as score
                FROM vectors v
                JOIN memory m ON v.id = m.id
                ORDER BY score DESC LIMIT ?
            """, (query_vec, limit)).fetchall()
            return [{"text": r[0], "type": r[1], "date": r[2], "score": r[3]} for r in results if r[3] >= threshold]

    def search_exact(self, keyword: str, limit: int = 5) -> List[Dict[str, Any]]:
        pattern = f"%{keyword}%"
        with self._lock:
            results = self.con.execute(
                "SELECT substr(text, 1, 500), memory_type, created_at FROM memory WHERE text ILIKE ? ORDER BY created_at DESC LIMIT ?",
                (pattern, limit)
            ).fetchall()
        return [{"text": r[0], "type": r[1], "date": r[2], "score": 1.0} for r in results]

    def get_triples(self, entity: str) -> List[tuple]:
        with self._lock:
            return self.con.execute(
                "SELECT subject, relation, object FROM triples WHERE subject=? OR object=?",
                (entity.lower(), entity.lower())
            ).fetchall()

    def get_active_plans(self) -> List[Dict[str, Any]]:
        with self._lock:
            res = self.con.execute("SELECT * FROM plans WHERE status = 'active'").fetchall()
        return [{"id": r[0], "goal": r[1], "steps": json.loads(r[2]), "status": r[3]} for r in res]

    def get_recent_memories(self, limit: int = 10) -> List[str]:
        with self._lock:
            res = self.con.execute("SELECT text FROM memory ORDER BY created_at DESC LIMIT ?", (limit,)).fetchall()
        return [r[0] for r in res]

    def calculate_similarities(self, query: str, texts: List[str]) -> List[float]:
        """
        Calculates cosine similarity between the query and a list of texts using
        the internal embedder. Returns a list of similarity scores.
        """
        if not texts:
            return []

        # 1. Embed all texts in one batch (query first)
        all_embeddings = list(self.embedder.embed([query] + texts))
        query_vec = all_embeddings[0]
        text_vecs = all_embeddings[1:]

        similarities = []
        for text_vec in text_vecs:
            # Cosine similarity for normalized vectors is the dot product.
            # Calculate dot product manually using list comprehension
            dot_product = sum(query_vec[i] * text_vec[i] for i in range(len(query_vec)))
            similarities.append(float(dot_product))

        return similarities


def get_memory_system(db_path: Optional[str] = None) -> MemoryStorage:
    # If path not provided, use config default
    path = db_path or config.MEMORY_DB_PATH
    return MemoryStorage(db_path=path)



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\memory\__init__.py
# ===========================================================================

from .storage import MemoryStorage, get_memory_system

MemorySystem = MemoryStorage

__all__ = ["MemoryStorage", "MemorySystem", "get_memory_system"]



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\perception\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\rag\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\conftest.py
# ===========================================================================

import sys
from unittest.mock import MagicMock
import pytest
import numpy as np

# Mock 'ocr' module globally to prevent circular imports as per memory instructions
sys.modules['Autonomous_Reasoning_System.tools.ocr'] = MagicMock()
# Also mock 'pypdf' if it's not installed in the environment but used in the code
# sys.modules['pypdf'] = MagicMock()

@pytest.fixture
def mock_embedding_model():
    """Returns a mock embedding model that returns fixed vectors."""
    mock = MagicMock()
    # Mock embed to return a generator of numpy arrays (embeddings)
    # We use a fixed size 384 as per config
    def side_effect(texts):
        for text in texts:
            # Return numpy array, which has .tolist()
            yield np.array([0.1] * 384)
    mock.embed.side_effect = side_effect
    return mock



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\integration\test_memory_integration.py
# ===========================================================================

import pytest
import os
import json
from Autonomous_Reasoning_System.memory.storage import MemoryStorage
from Autonomous_Reasoning_System import config

@pytest.fixture
def memory_db():
    # Use in-memory DB for integration tests to avoid file I/O and state persistence issues
    storage = MemoryStorage(db_path=":memory:")
    yield storage

def test_memory_integration_flow(memory_db):
    """
    Test a full flow: Remember -> Search -> Verify
    """
    # 1. Remember some data
    text1 = "The capital of France is Paris."
    text2 = "The capital of Germany is Berlin."
    memory_db.remember(text1, metadata={"category": "geography"})
    memory_db.remember(text2, metadata={"category": "geography"})

    # 2. Search Similar
    # NOTE: The default `search_similar` threshold is 0.4.
    # If using real embeddings, "France capital" matches "The capital of France is Paris." quite well.
    # However, if something is wrong with VSS or embedding calculation, it might return 0 results.

    # Let's lower the threshold to 0.0 to ensure we get results if the extension works at all.
    results = memory_db.search_similar("France capital", threshold=0.0)

    # Check if we got results
    if len(results) == 0:
        # Debugging: check if vectors table has entries
        count = memory_db.con.execute("SELECT count(*) FROM vectors").fetchone()[0]
        print(f"\n[DEBUG] Vectors count: {count}")

        # Check if vss extension is working by running a simple query
        try:
            memory_db.con.execute("SELECT list_cosine_similarity([1,2,3], [1,2,3])").fetchall()
            print("[DEBUG] VSS function works.")
        except Exception as e:
            print(f"[DEBUG] VSS function failed: {e}")

    assert len(results) >= 1
    found_texts = [r['text'] for r in results]
    assert text1 in found_texts or text2 in found_texts

def test_exact_search_integration(memory_db):
    """Test exact search works with the DB."""
    text = "UniqueKeyword123 is here."
    memory_db.remember(text)

    results = memory_db.search_exact("UniqueKeyword123")
    assert len(results) == 1
    assert results[0]['text'] == text

def test_document_reassembly_integration(memory_db):
    """Test storing multiple chunks and retrieving them as a document."""
    filename = "doc_integration.txt"
    chunks = ["Chunk 1.", "Chunk 2.", "Chunk 3."]

    metas = [{"filename": filename} for _ in chunks]

    memory_db.remember_batch(chunks, source=filename, metadata_list=metas)

    full_text = memory_db.get_whole_document(filename)
    assert full_text == "Chunk 1.\nChunk 2.\nChunk 3."

def test_kg_triples_integration(memory_db):
    """Test adding and retrieving triples."""
    memory_db.add_triple("Alice", "knows", "Bob")
    memory_db.add_triple("Bob", "knows", "Charlie")

    triples = memory_db.get_triples("Bob")
    # Should get (Alice, knows, Bob) and (Bob, knows, Charlie)
    assert len(triples) == 2

    # Verify content
    subjects = [t[0] for t in triples]
    objects = [t[2] for t in triples]
    assert "alice" in subjects or "bob" in subjects # lowercased in DB
    assert "bob" in objects or "charlie" in objects

def test_plan_persistence(memory_db):
    """Test that plans are saved and retrieved correctly."""
    plan_id = "integration_plan"
    goal = "integration testing"
    steps = [{"id": 1, "desc": "step 1"}]

    memory_db.update_plan(plan_id, goal, steps)

    plans = memory_db.get_active_plans()
    found = next((p for p in plans if p['id'] == plan_id), None)

    assert found is not None
    assert found['goal'] == goal
    assert found['steps'] == steps



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\integration\test_web_search_integration.py
# ===========================================================================

import pytest
from Autonomous_Reasoning_System.tools.web_search import perform_google_search

# We use a marker or check if we are in an environment that supports this
# For now, we just try it. If it fails due to network or missing browser, it fails.
# But to be safe for CI, we usually mark it.
# However, the user asked "does it work?", so we want to run it.

def test_web_search_integration():
    """
    Integration test for web search.
    Requires: Internet access and Playwright browsers installed.
    """
    query = "Python programming language"
    result = perform_google_search(query)

    # Check if we got a valid response (not error string)
    assert "Error performing search" not in result

    # If network is down or google blocks, it might return "No search results found." or error.
    # But if it works, it should contain "Python"
    if "No search results found" not in result:
        assert "Python" in result or "programming" in result
    else:
        # If no results, print warning but don't fail if it's just a network glitch?
        # But for "does it work", getting no results for "Python" implies it doesn't work well.
        pytest.skip("No search results found - might be network issue or blocking.")



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\integration\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\unit\test_memory_storage.py
# ===========================================================================

import pytest
from unittest.mock import MagicMock, patch, ANY
import json
from Autonomous_Reasoning_System.memory.storage import MemoryStorage

@pytest.fixture
def mock_duckdb():
    with patch('duckdb.connect') as mock_connect:
        mock_con = MagicMock()
        mock_connect.return_value = mock_con
        yield mock_con

@pytest.fixture
def memory_storage(mock_duckdb, mock_embedding_model):
    with patch('Autonomous_Reasoning_System.memory.storage.TextEmbedding', return_value=mock_embedding_model):
        storage = MemoryStorage(db_path=":memory:")
        return storage

def test_init_schema(memory_storage, mock_duckdb):
    """Test that schema initialization commands are executed."""
    # We expect several execute calls for schema creation
    # checking for 'CREATE TABLE IF NOT EXISTS memory'
    calls = [args[0] for args, _ in mock_duckdb.execute.call_args_list]
    assert any("CREATE TABLE IF NOT EXISTS memory" in call for call in calls)
    assert any("CREATE TABLE IF NOT EXISTS vectors" in call for call in calls)
    assert any("CREATE TABLE IF NOT EXISTS triples" in call for call in calls)
    assert any("CREATE TABLE IF NOT EXISTS plans" in call for call in calls)

def test_remember_batch(memory_storage, mock_duckdb):
    """Test remember_batch inserts data into memory and vectors tables."""
    texts = ["hello world", "another memory"]
    metadata_list = [{"meta": 1}, {"meta": 2}]

    memory_storage.remember_batch(texts, metadata_list=metadata_list)

    # Verify transaction calls
    mock_duckdb.execute.assert_any_call("BEGIN TRANSACTION")
    mock_duckdb.execute.assert_any_call("COMMIT")

    # Verify insert calls
    # We can inspect the arguments passed to execute
    # INSERT INTO memory ...
    insert_memory_calls = [
        call for call in mock_duckdb.execute.call_args_list
        if "INSERT INTO memory VALUES" in call[0][0]
    ]
    assert len(insert_memory_calls) == 2

    # INSERT INTO vectors ...
    insert_vector_calls = [
        call for call in mock_duckdb.execute.call_args_list
        if "INSERT INTO vectors VALUES" in call[0][0]
    ]
    assert len(insert_vector_calls) == 2

def test_get_whole_document(memory_storage, mock_duckdb):
    """Test retrieving and reassembling a document."""
    filename = "test_doc.txt"

    # Mock return value of fetchall
    mock_duckdb.execute.return_value.fetchall.return_value = [
        ("Part 1 content",),
        ("Part 2 content",)
    ]

    result = memory_storage.get_whole_document(filename)

    assert result == "Part 1 content\nPart 2 content"

    # Verify query
    args, _ = mock_duckdb.execute.call_args
    query = args[0]
    assert "SELECT text" in query
    assert "WHERE source = ?" in query
    assert "ORDER BY created_at ASC" in query

def test_add_triple(memory_storage, mock_duckdb):
    """Test adding a triple."""
    memory_storage.add_triple("Subj", "Rel", "Obj")
    mock_duckdb.execute.assert_called_with(
        "INSERT OR IGNORE INTO triples VALUES (?, ?, ?)",
        ("subj", "rel", "obj")
    )

def test_update_plan_new(memory_storage, mock_duckdb):
    """Test creating a new plan."""
    # Mock select to return None (plan doesn't exist)
    mock_duckdb.execute.return_value.fetchone.return_value = None

    plan_id = "plan1"
    goal = "do something"
    steps = ["step1", "step2"]

    memory_storage.update_plan(plan_id, goal, steps)

    # Verify insert
    args_list = mock_duckdb.execute.call_args_list
    insert_call = [call for call in args_list if "INSERT INTO plans VALUES" in call[0][0]]
    assert len(insert_call) == 1

def test_update_plan_existing(memory_storage, mock_duckdb):
    """Test updating an existing plan."""
    # Mock select to return True (plan exists)
    mock_duckdb.execute.return_value.fetchone.return_value = (1,)

    plan_id = "plan1"
    goal = "do something"
    steps = ["step1", "step2"]

    memory_storage.update_plan(plan_id, goal, steps)

    # Verify update
    args_list = mock_duckdb.execute.call_args_list
    update_call = [call for call in args_list if "UPDATE plans SET" in call[0][0]]
    assert len(update_call) == 1

def test_search_similar(memory_storage, mock_duckdb):
    """Test similarity search."""
    # Mock fetchall result
    mock_duckdb.execute.return_value.fetchall.return_value = [
        ("Result text", "episodic", "2023-01-01", 0.9)
    ]

    results = memory_storage.search_similar("query")

    assert len(results) == 1
    assert results[0]["text"] == "Result text"
    assert results[0]["score"] == 0.9

def test_search_exact(memory_storage, mock_duckdb):
    """Test exact keyword search."""
    mock_duckdb.execute.return_value.fetchall.return_value = [
        ("Exact match", "episodic", "2023-01-01")
    ]

    results = memory_storage.search_exact("keyword")

    assert len(results) == 1
    assert results[0]["text"] == "Exact match"
    assert results[0]["score"] == 1.0



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\unit\test_router_web_search.py
# ===========================================================================


import pytest
from unittest.mock import MagicMock
from Autonomous_Reasoning_System.control.router import Router, IntentFamily

def test_router_web_search_resolution():
    """
    Verifies that web search related queries are correctly routed to the WEB_SEARCH family
    and the 'google_search' pipeline.
    """
    mock_dispatcher = MagicMock()

    # Setup mock dispatcher to return specific intent for testing
    # In a real scenario, analyze_intent would return 'google', 'web_search', etc.
    def mock_dispatch(tool_name, arguments=None):
        if tool_name == "analyze_intent":
            text = arguments.get("text", "").lower()
            if "google" in text:
                return {"status": "success", "data": {"intent": "google", "family": "web_search"}}
            if "search" in text:
                return {"status": "success", "data": {"intent": "web_search", "family": "web_search"}}
            return {"status": "success", "data": {"intent": "unknown"}}
        return {"status": "success", "data": {}}

    mock_dispatcher.dispatch.side_effect = mock_dispatch

    router = Router(dispatcher=mock_dispatcher)

    # Test cases
    test_queries = [
        ("google python 3.12 release date", "google"),
        ("search web for autonomous agents", "web_search"),
    ]

    for query, expected_intent in test_queries:
        result = router.resolve(query)

        assert result["intent"] == expected_intent
        assert result["family"] == IntentFamily.WEB_SEARCH
        assert result["pipeline"] == ["google_search"]

def test_router_web_search_fallback():
    """
    Verifies that if the intent analyzer misses the family but catches the intent,
    the Router still maps it correctly using its internal map.
    """
    mock_dispatcher = MagicMock()

    # Analyzer returns correct intent but NO family
    def mock_dispatch(tool_name, arguments=None):
        if tool_name == "analyze_intent":
            return {"status": "success", "data": {"intent": "google", "family": "unknown"}}
        return {"status": "success", "data": {}}

    mock_dispatcher.dispatch.side_effect = mock_dispatch

    router = Router(dispatcher=mock_dispatcher)

    result = router.resolve("google something")

    assert result["intent"] == "google"
    # Router should use its internal map to find the family for 'google'
    assert result["family"] == IntentFamily.WEB_SEARCH
    assert result["pipeline"] == ["google_search"]



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\unit\test_web_search.py
# ===========================================================================

import pytest
from unittest.mock import MagicMock, patch
from Autonomous_Reasoning_System.tools.web_search import perform_google_search

@pytest.fixture
def mock_playwright():
    with patch('Autonomous_Reasoning_System.tools.web_search.sync_playwright') as mock:
        yield mock

def test_perform_google_search_success(mock_playwright):
    """Test successful google search with results."""
    # Setup mock chain
    mock_p = mock_playwright.return_value.__enter__.return_value
    mock_browser = mock_p.chromium.launch.return_value
    mock_page = mock_browser.new_context.return_value.new_page.return_value

    # Mock evaluate return values
    # First call is main results, Second call is quick answer
    # But wait, evaluate is called twice in the code.
    # 1. results extraction
    # 2. quick answer extraction

    expected_results = [
        "Title: Test Title\nLink: http://example.com\nSnippet: Test Snippet"
    ]

    def evaluate_side_effect(script):
        if "document.querySelectorAll('.g')" in script:
            return expected_results
        if "document.querySelector('.Iz6qV')" in script:
            return "Quick Answer: 42"
        return None

    mock_page.evaluate.side_effect = evaluate_side_effect

    result = perform_google_search("test query")

    # Verify browser interaction
    mock_page.goto.assert_called()
    assert "google.com/search?q=test+query" in mock_page.goto.call_args[0][0]

    # Verify results
    assert "Quick Answer: 42" in result
    assert "Title: Test Title" in result
    assert "Snippet: Test Snippet" in result

def test_perform_google_search_no_results(mock_playwright):
    """Test search with no results found."""
    mock_p = mock_playwright.return_value.__enter__.return_value
    mock_page = mock_p.chromium.launch.return_value.new_context.return_value.new_page.return_value

    mock_page.evaluate.return_value = [] # No results for both calls

    result = perform_google_search("weird query")

    assert result == "No search results found."

def test_perform_google_search_error(mock_playwright):
    """Test search when exception occurs."""
    mock_p = mock_playwright.return_value.__enter__.return_value
    mock_browser = mock_p.chromium.launch.return_value
    # Make launching raise an error
    mock_browser.new_context.side_effect = Exception("Browser crashed")

    result = perform_google_search("crash me")

    assert "Error performing search" in result
    assert "Browser crashed" in result



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tests\unit\__init__.py
# ===========================================================================




# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\action_executor.py
# ===========================================================================

# tools/action_executor.py
"""
ActionExecutor
---------------
Bridges Tyrone's planning system to external tools or cognitive functions.
Given a step description, it resolves an appropriate tool and executes it.
"""

from Autonomous_Reasoning_System.memory.storage import MemoryStorage


class ActionExecutor:
    def __init__(self, memory_storage=None):
        self.memory = memory_storage or MemoryStorage()

    # ---------------------- Dispatcher ----------------------
    def execute_step(self, step_description: str, workspace) -> dict:
        """
        Attempt to execute a single plan step based on its description.
        Returns a structured result dict.
        """
        text = step_description.lower()
        result_text = "No matching tool found."
        success = False

        try:
            if "ocr" in text or "extract text" in text:
                try:
                    from Autonomous_Reasoning_System.tools import ocr  # optional dependency
                    image_path = workspace.get("image_path", "data/sample_image.jpg")
                    extracted = ocr.run(image_path)
                    workspace.set("extracted_text", extracted)
                    result_text = f"OCR extracted text of length {len(extracted)}"
                    success = True
                except Exception as e:
                    result_text = f"OCR tool unavailable: {e}"

            elif "load image" in text:
                # Placeholder for an image load step
                workspace.set("image_path", "data/sample_image.jpg")
                result_text = "Loaded sample image successfully."
                success = True

            elif "store" in text and "text" in text:
                text_to_store = workspace.get("extracted_text", "")
                if text_to_store:
                    self.memory.add_memory(
                        text=f"Stored OCR text snippet: {text_to_store[:80]}",
                        memory_type="ocr_result",
                        importance=0.5,
                        source="ActionExecutor",
                    )
                    result_text = "Stored OCR text in long-term memory."
                    success = True

        except Exception as e:
            result_text = f"Error executing step: {e}"

        # Log the attempt
        self.memory.add_memory(
            text=f"Action executed: '{step_description}' -> {result_text}",
            memory_type="action_log",
            importance=0.3,
            source="ActionExecutor",
        )

        return {"success": success, "result": result_text}



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\deterministic_responder.py
# ===========================================================================

import datetime
import math
import requests


class DeterministicResponder:
    """
    Handles factual, numeric, and system-level queries without invoking the LLM.
    Uses local logic or small public lookups.
    """

    def run(self, text: str) -> str:
        q = text.lower().strip()

        # --- date/time ---
        if any(k in q for k in ["time", "date", "today", "now"]):
            return datetime.datetime.now().strftime("%A, %d %B %Y %H:%M:%S")

        # --- math ---
        try:
            if any(op in q for op in ["+", "-", "*", "/"]) and all(
                c.isdigit() or c.isspace() or c in "+-*/." for c in q
            ):
                return str(eval(q))
        except Exception:
            pass

        # --- factual lookup (Wikipedia REST API) ---
        try:
            resp = requests.get(
                f"https://en.wikipedia.org/api/rest_v1/page/summary/{q.replace(' ', '_')}",
                timeout=5,
            )
            if resp.ok:
                data = resp.json()
                if "extract" in data:
                    return data["extract"]
        except Exception:
            pass

        return "I'm not sure, but I can look it up later."



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\entity_extractor.py
# ===========================================================================

import json
import re
from Autonomous_Reasoning_System.llm.engine import call_llm

class EntityExtractor:
    """
    Extracts key search entities from a natural language query.
    Designed to support deterministic retrieval by identifying the core subjects.
    """

    def __init__(self):
        self.system_prompt = (
            "You are a query entity extractor. "
            "Your goal is to extract the core subject and keywords from a user's question for a search engine. "
            "Ignore question words (Who, What, Where, When, How) and auxiliary verbs. "
            "Focus on proper nouns, specific nouns, and key actions. "
            "Return a JSON list of strings. "
            "Example: 'When is Cornelia's birthday?' -> [\"Cornelia\", \"birthday\"] "
            "Example: 'Show me the project plan for Alpha' -> [\"project plan\", \"Alpha\"] "
            "Respond ONLY with the JSON list."
        )

    def extract(self, text: str) -> list[str]:
        """
        Extract keywords from the text.
        """
        try:
            raw = call_llm(system_prompt=self.system_prompt, user_prompt=text)
            # clean up potential markdown blocks
            raw = re.sub(r"```json|```", "", raw).strip()

            # Attempt to parse JSON
            keywords = json.loads(raw)

            if isinstance(keywords, list):
                return [str(k) for k in keywords]
            else:
                return []
        except Exception as e:
            print(f"[EntityExtractor] Extraction failed: {e}")
            # Fallback: simple split/filter if LLM fails (basic heuristic)
            words = text.split()
            filtered = [w for w in words if w.lower() not in ["who", "what", "where", "when", "how", "is", "the", "a", "an", "in", "on", "at", "for"]]
            return filtered



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\standard_tools.py
# ===========================================================================

import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

def register_tools(dispatcher, components: Dict[str, Any]):
    """
    Registers standard tools with the dispatcher, binding them to the provided components.

    Args:
        dispatcher: The Dispatcher instance.
        components: A dictionary containing instances of system components:
            - "intent_analyzer"
            - "memory"
            - "reflector"
            - "plan_builder"
            - "deterministic_responder" (optional, if not present, will be created)
            - "context_adapter" (optional)
            - "goal_manager" (optional)
    """

    def _format_results(results) -> str:
        """Pretty-print list/dict results for console readability."""
        if results is None:
            return "No results."
        if isinstance(results, str):
            return results
        if isinstance(results, list):
            if not results:
                return "No results."
            lines = []
            for r in results:
                if isinstance(r, dict) and "text" in r:
                    lines.append(f"- {r['text']}")
                else:
                    lines.append(f"- {r}")
            return "\n".join(lines)
        return str(results)

    # 1. Analyze Intent
    def analyze_intent_handler(text: str, **kwargs):
        analyzer = components.get("intent_analyzer")
        if analyzer:
            return analyzer.analyze(text)
        return {"intent": "unknown"}

    dispatcher.register_tool(
        "analyze_intent",
        analyze_intent_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 2. Store Memory
    def store_memory_handler(text: str, **kwargs):
        memory = components.get("memory")
        if memory:
            memory.remember(
                text=f"Stored fact: {text}",
                metadata={"type": "personal_fact", "importance": 1.0, "source": "tool:store_memory"}
            )
            return f"Stored: {text}"
        return "Memory component not available."

    dispatcher.register_tool(
        "store_memory",
        store_memory_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 3. Search Memory
    def search_memory_handler(text: str, **kwargs):
        memory = components.get("memory")
        if memory:
            # Assuming retrieve method exists and returns list
            results = memory.retrieve(text, k=3)
            return _format_results(results)
        return "Memory component not available."

    dispatcher.register_tool(
        "search_memory",
        search_memory_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 4. Perform Reflection
    def perform_reflection_handler(text: str, **kwargs):
        reflector = components.get("reflector")
        if reflector:
            return reflector.interpret(text)
        return "Reflector component not available."

    dispatcher.register_tool(
        "perform_reflection",
        perform_reflection_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 5. Summarize Context
    def summarize_context_handler(text: str, **kwargs):
        # This might use memory summarization or just reflection
        reflector = components.get("reflector")
        if reflector:
            return reflector.interpret(f"Summarize this: {text}")
        return "Reflector component not available."

    dispatcher.register_tool(
        "summarize_context",
        summarize_context_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 6. Deterministic Responder
    def deterministic_responder_handler(text: str, **kwargs):
        responder = components.get("deterministic_responder")
        if not responder:
            from Autonomous_Reasoning_System.tools.deterministic_responder import DeterministicResponder
            responder = DeterministicResponder()
        return responder.run(text)

    dispatcher.register_tool(
        "deterministic_responder",
        deterministic_responder_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 7. Plan Steps (decompose goal)
    def plan_steps_handler(text: str, goal: str = None, **kwargs):
        plan_builder = components.get("plan_builder")
        target_text = goal or text
        if plan_builder:
            steps = plan_builder.decompose_goal(target_text)
            # Pretty-print steps for console use
            return "\n".join(f"{i+1}. {s}" for i, s in enumerate(steps)) if steps else "No steps generated."
        return ["No PlanBuilder available"]

    dispatcher.register_tool(
        "plan_steps",
        plan_steps_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 8. Answer Question (Generic LLM / ContextAdapter)
    def answer_question_handler(text: str, **kwargs):
        adapter = components.get("context_adapter")
        if not adapter:
            # Lazy import if not provided
            try:
                from Autonomous_Reasoning_System.llm.context_adapter import ContextAdapter
                adapter = ContextAdapter()
            except ImportError:
                pass

        if adapter:
            return adapter.run(text)
        return "I cannot answer that right now."

    dispatcher.register_tool(
        "answer_question",
        answer_question_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 9. Goal Management
    def create_goal_handler(text: str, priority: int = 1, **kwargs):
        goal_manager = components.get("goal_manager")
        if goal_manager:
            return goal_manager.create_goal(text, priority=priority)
        return "Goal Manager not available."

    dispatcher.register_tool(
        "create_goal",
        create_goal_handler,
        schema={
            "text": {"type": str, "required": True},
            "priority": {"type": int, "required": False}
        }
    )

    def list_goals_handler(status: str = None, **kwargs):
        goal_manager = components.get("goal_manager")
        if goal_manager:
             # We access memory directly or via goal manager helper
             # GoalManager doesn't expose list directly, but memory does
             active_goals = goal_manager.memory.get_active_goals()
             if active_goals.empty:
                 return "No active goals."

             if status:
                 active_goals = active_goals[active_goals['status'] == status]
                 if active_goals.empty:
                     return f"No goals with status '{status}'."

             summary = []
             for _, row in active_goals.iterrows():
                 summary.append(f"[{row['id'][:8]}] {row['text']} (Status: {row['status']})")
             return "\n".join(summary)
        return "Goal Manager not available."

    dispatcher.register_tool(
        "list_goals",
        list_goals_handler,
        schema={
            "status": {"type": str, "required": False}
        }
    )

    # --- NEW CONTROLLER TOOLS FOR FAMILIES ---

    # 10. Handle Memory Ops (Unified)
    def handle_memory_ops_handler(text: str, intent: str = None, context: Dict[str, Any] = None, **kwargs):
        # Check intent from args or context if available
        effective_intent = intent
        if not effective_intent and context:
             effective_intent = context.get("intent", "unknown")

        if not effective_intent:
             effective_intent = "unknown"

        memory = components.get("memory")

        if not memory:
            return "Memory component not available."

        # Dispatch based on intent
        if effective_intent in ["store", "save", "remind", "remember", "memorize", "note"]:
            memory.remember(
                text=text.strip(),
                metadata={"type": "personal_fact", "importance": 1.0, "source": "explicit_user_command"}
            )
            return f"Got it ‚Äî I will remember: {text}"
        elif effective_intent in ["search", "recall", "find", "lookup"]:
            results = memory.retrieve(text, k=3)
            return _format_results(results)
        else:
            # Default to search if unknown intent in memory family, but do not auto-store
            results = memory.retrieve(text, k=3)
            return _format_results(results)

    dispatcher.register_tool(
        "handle_memory_ops",
        handle_memory_ops_handler,
        schema={
            "text": {"type": str, "required": True},
            "intent": {"type": str, "required": False}
        }
    )

    # 11. Handle Goal Ops (Unified)
    def handle_goal_ops_handler(text: str, context: Dict[str, Any] = None, **kwargs):
        intent = context.get("intent", "unknown") if context else "unknown"
        goal_manager = components.get("goal_manager")

        if not goal_manager:
             return "Goal Manager not available."

        if intent in ["list_goals", "goals"]:
             # Reuse logic or call internal method
             # Just calling the helper for now:
             active_goals = goal_manager.memory.get_active_goals()
             if active_goals.empty:
                 return "No active goals."
             summary = []
             for _, row in active_goals.iterrows():
                 summary.append(f"[{row['id'][:8]}] {row['text']} (Status: {row['status']})")
             return "\n".join(summary)
        else:
             # Default to creating goal for other intents (create_goal, achieve, do, task, research, investigate, etc.)
             return goal_manager.create_goal(text)

    dispatcher.register_tool(
        "handle_goal_ops",
        handle_goal_ops_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 12. Perform Self Analysis
    def perform_self_analysis_handler(text: str, **kwargs):
        reflector = components.get("reflector")
        if reflector:
             # We might want to check system status here
             return reflector.interpret(f"Analyze system status and self: {text}")
        return "Reflector not available."

    dispatcher.register_tool(
        "perform_self_analysis",
        perform_self_analysis_handler,
        schema={"text": {"type": str, "required": True}}
    )

    # 13. Google Search
    def google_search_handler(text: str, **kwargs):
        """
        Handler for Google Search intent using Playwright.
        """
        from Autonomous_Reasoning_System.tools.web_search import perform_google_search
        return perform_google_search(text)

    dispatcher.register_tool(
        "google_search",
        google_search_handler,
        schema={"text": {"type": str, "required": True}}
    )


    logger.info("Standard tools registered successfully.")



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\system_tools.py
# ===========================================================================

import datetime
import requests
import logging

logger = logging.getLogger(__name__)

def get_current_time() -> str:
    """Returns the current date and time as a string."""
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def get_current_location() -> str:
    """Returns the current location based on IP address."""
    try:
        response = requests.get("http://ip-api.com/json/", timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get("status") == "success":
                city = data.get("city", "Unknown City")
                country = data.get("country", "Unknown Country")
                region_name = data.get("regionName", "")
                return f"{city}, {region_name}, {country}"
    except Exception as e:
        logger.warning(f"Failed to retrieve location: {e}")

    return "Location unavailable"



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\web_search.py
# ===========================================================================


import logging
from playwright.sync_api import sync_playwright
import urllib.parse
import time

logger = logging.getLogger(__name__)

def perform_google_search(query: str, **kwargs) -> str:
    """
    Performs a Google search using Playwright and returns the top results.
    """
    logger.info(f"Performing Google Search for: {query}")

    with sync_playwright() as p:
        try:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            )
            page = context.new_page()

            encoded_query = urllib.parse.quote_plus(query)
            url = f"https://www.google.com/search?q={encoded_query}"

            page.goto(url, timeout=30000)

            # Wait for results to load
            try:
                page.wait_for_selector("#search", timeout=5000)
            except:
                logger.warning("Google search selector timeout")

            # Extract text from search results
            # We target standard result blocks
            results = []

            # Get main snippet
            # Using evaluate to get text content safer
            js_extract = """
            () => {
                const items = Array.from(document.querySelectorAll('.g'));
                return items.map(item => {
                    const title = item.querySelector('h3')?.innerText || '';
                    const snippet = item.querySelector('.VwiC3b')?.innerText || item.querySelector('.IsZvec')?.innerText || '';
                    const link = item.querySelector('a')?.href || '';
                    if (title && snippet) {
                        return `Title: ${title}\\nLink: ${link}\\nSnippet: ${snippet}`;
                    }
                    return null;
                }).filter(i => i !== null).slice(0, 5);
            }
            """

            extracted_items = page.evaluate(js_extract)

            if extracted_items:
                results.extend(extracted_items)

            # Also check for "featured snippet" or "knowledge panel"
            # .kp-header or similar
            # .xpdopen (sometimes used for quick answers)

            js_quick_answer = """
            () => {
                const answer = document.querySelector('.Iz6qV')?.innerText || document.querySelector('.hgKElc')?.innerText;
                if (answer) return "Quick Answer: " + answer;
                return null;
            }
            """
            quick_answer = page.evaluate(js_quick_answer)
            if quick_answer:
                results.insert(0, quick_answer)

            browser.close()

            if not results:
                return "No search results found."

            return "\n\n".join(results)

        except Exception as e:
            logger.error(f"Google Search failed: {e}")
            return f"Error performing search: {e}"



# ===========================================================================
# FILE START: Autonomous_Reasoning_System\tools\__init__.py
# ===========================================================================

"""
Expose the concrete tool implementations that actually exist in this package.
The earlier template imports referenced modules that were never added to the
repository, which resulted in ImportError as soon as the package was imported.
"""

from .action_executor import ActionExecutor
from .deterministic_responder import DeterministicResponder

__all__ = [
    "ActionExecutor",
    "DeterministicResponder",
]


